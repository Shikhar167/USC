{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saira\\anaconda3\\envs\\CSCLDL\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import typing\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBBertDataset(Dataset):\n",
    "    # Define Special tokens as attributes of class\n",
    "    CLS = '[CLS]'\n",
    "    PAD = '[PAD]'\n",
    "    SEP = '[SEP]'\n",
    "    MASK = '[MASK]'\n",
    "    UNK = '[UNK]'\n",
    "\n",
    "    MASK_PERCENTAGE = 0.15  # How much words to mask\n",
    "\n",
    "    MASKED_INDICES_COLUMN = 'masked_indices'\n",
    "    TARGET_COLUMN = 'indices'\n",
    "    NSP_TARGET_COLUMN = 'is_next'\n",
    "    TOKEN_MASK_COLUMN = 'token_mask'\n",
    "\n",
    "    OPTIMAL_LENGTH_PERCENTILE = 70\n",
    "\n",
    "    def __init__(self, path, ds_from=None, ds_to=None, should_include_text=False):\n",
    "        self.ds: pd.Series = pd.read_csv(path)['review']\n",
    "\n",
    "        if ds_from is not None or ds_to is not None:\n",
    "            self.ds = self.ds[ds_from:ds_to]\n",
    "\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        self.counter = Counter()\n",
    "        self.vocab = None\n",
    "\n",
    "        self.optimal_sentence_length = None\n",
    "        self.should_include_text = should_include_text\n",
    "\n",
    "        if should_include_text:\n",
    "            self.columns = ['masked_sentence', self.MASKED_INDICES_COLUMN, 'sentence', self.TARGET_COLUMN,\n",
    "                            self.TOKEN_MASK_COLUMN,\n",
    "                            self.NSP_TARGET_COLUMN]\n",
    "        else:\n",
    "            self.columns = [self.MASKED_INDICES_COLUMN, self.TARGET_COLUMN, self.TOKEN_MASK_COLUMN,\n",
    "                            self.NSP_TARGET_COLUMN]\n",
    "        self.df = pd.DataFrame(self.prepare_dataset())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.iloc[idx]\n",
    "\n",
    "        inp = torch.Tensor(item[self.MASKED_INDICES_COLUMN]).long()\n",
    "        token_mask = torch.Tensor(item[self.TOKEN_MASK_COLUMN]).bool()\n",
    "\n",
    "        mask_target = torch.Tensor(item[self.TARGET_COLUMN]).long()\n",
    "        mask_target = mask_target.masked_fill_(token_mask, 0)\n",
    "\n",
    "        attention_mask = (inp == self.vocab[self.PAD]).unsqueeze(0)\n",
    "\n",
    "        if item[self.NSP_TARGET_COLUMN] == 0:\n",
    "            t = [1, 0]\n",
    "        else:\n",
    "            t = [0, 1]\n",
    "\n",
    "        nsp_target = torch.Tensor(t)\n",
    "\n",
    "        return (\n",
    "            inp.to(device),\n",
    "            attention_mask.to(device),\n",
    "            token_mask.to(device),\n",
    "            mask_target.to(device),\n",
    "            nsp_target.to(device)\n",
    "        )\n",
    "    \n",
    "    def prepare_dataset(self) -> pd.DataFrame:\n",
    "        sentences = []\n",
    "        nsp = []\n",
    "        sentence_lens = []\n",
    "\n",
    "        # Split dataset on sentences\n",
    "        for review in self.ds:\n",
    "            review_sentences = review.split('. ')\n",
    "            sentences += review_sentences\n",
    "            self._update_length(review_sentences, sentence_lens)\n",
    "        self.optimal_sentence_length = self._find_optimal_sentence_length(sentence_lens)\n",
    "\n",
    "        print(\"Create vocabulary\")\n",
    "        for sentence in tqdm(sentences):\n",
    "            s = self.tokenizer(sentence)\n",
    "            self.counter.update(s)\n",
    "\n",
    "        self._fill_vocab()\n",
    "\n",
    "        print(\"Preprocessing dataset\")\n",
    "        for review in tqdm(self.ds):\n",
    "            review_sentences = review.split('. ')\n",
    "            if len(review_sentences) > 1:\n",
    "                for i in range(len(review_sentences) - 1):\n",
    "                    # True NSP item\n",
    "                    first, second = self.tokenizer(review_sentences[i]), self.tokenizer(review_sentences[i + 1])\n",
    "\n",
    "                    nsp.append(self._create_item(first, second, 1))\n",
    "\n",
    "                    # False NSP item\n",
    "                    first, second = self._select_false_nsp_sentences(sentences)\n",
    "                    first, second = self.tokenizer(first), self.tokenizer(second)\n",
    "                    nsp.append(self._create_item(first, second, 0))\n",
    "        df = pd.DataFrame(nsp, columns=self.columns)\n",
    "        return df  \n",
    "    \n",
    "    def _update_length(self, sentences: typing.List[str], lengths: typing.List[int]):\n",
    "        for v in sentences:\n",
    "            l = len(v.split())\n",
    "            lengths.append(l)\n",
    "        return lengths\n",
    "\n",
    "    def _find_optimal_sentence_length(self, lengths: typing.List[int]):\n",
    "        arr = np.array(lengths)\n",
    "        return int(np.percentile(arr, self.OPTIMAL_LENGTH_PERCENTILE))\n",
    "\n",
    "    def _fill_vocab(self):\n",
    "        # specials= argument is only in 0.12.0 version\n",
    "        # specials=[self.CLS, self.PAD, self.MASK, self.SEP, self.UNK]\n",
    "        self.vocab = vocab(self.counter, min_freq=2)\n",
    "\n",
    "        # 0.11.0 uses this approach to insert specials\n",
    "        self.vocab.insert_token(self.CLS, 0)\n",
    "        self.vocab.insert_token(self.PAD, 1)\n",
    "        self.vocab.insert_token(self.MASK, 2)\n",
    "        self.vocab.insert_token(self.SEP, 3)\n",
    "        self.vocab.insert_token(self.UNK, 4)\n",
    "        self.vocab.set_default_index(4)  \n",
    "\n",
    "    def _create_item(self, first: typing.List[str], second: typing.List[str], target: int = 1):\n",
    "        # Create masked sentence item\n",
    "        updated_first, first_mask = self._preprocess_sentence(first.copy())\n",
    "        updated_second, second_mask = self._preprocess_sentence(second.copy())\n",
    "        nsp_sentence = updated_first + [self.SEP] + updated_second\n",
    "        nsp_indices = self.vocab.lookup_indices(nsp_sentence)\n",
    "        inverse_token_mask = first_mask + [True] + second_mask\n",
    "\n",
    "        # Create sentence item without masking random words\n",
    "        first, _ = self._preprocess_sentence(first.copy(), should_mask=False)\n",
    "        second, _ = self._preprocess_sentence(second.copy(), should_mask=False)\n",
    "\n",
    "        original_nsp_sentence = first + [self.SEP] + second\n",
    "        original_nsp_indices = self.vocab.lookup_indices(original_nsp_sentence)\n",
    "\n",
    "        if self.should_include_text:\n",
    "            return (\n",
    "                nsp_sentence,\n",
    "                nsp_indices,\n",
    "                original_nsp_sentence,\n",
    "                original_nsp_indices,\n",
    "                inverse_token_mask,\n",
    "                target\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                nsp_indices,\n",
    "                original_nsp_indices,\n",
    "                inverse_token_mask,\n",
    "                target\n",
    "            )\n",
    "        \n",
    "    def _select_false_nsp_sentences(self, sentences: typing.List[str]):\n",
    "        \"\"\"Select sentences to create false NSP item\n",
    "\n",
    "        Args:\n",
    "            sentences: list of all sentences\n",
    "\n",
    "        Returns:\n",
    "            tuple of two sentences. The second one NOT the next sentence\n",
    "        \"\"\"\n",
    "        sentences_len = len(sentences)\n",
    "        sentence_index = random.randint(0, sentences_len - 1)\n",
    "        next_sentence_index = random.randint(0, sentences_len - 1)\n",
    "\n",
    "        # To be sure that it's not real next sentence\n",
    "        while next_sentence_index == sentence_index + 1:\n",
    "            next_sentence_index = random.randint(0, sentences_len - 1)\n",
    "\n",
    "        return sentences[sentence_index], sentences[next_sentence_index]\n",
    "\n",
    "    def _preprocess_sentence(self, sentence: typing.List[str], should_mask: bool = True):\n",
    "        inverse_token_mask = None\n",
    "        if should_mask == True:\n",
    "            sentence, inverse_token_mask = self._mask_sentence(sentence)\n",
    "            sentence, inverse_token_mask = self._pad_sentence([self.CLS] + sentence, [True] + inverse_token_mask)\n",
    "       \n",
    "        return sentence, inverse_token_mask\n",
    "\n",
    "    def _mask_sentence(self, sentence: typing.List[str]):\n",
    "        \"\"\"Replace MASK_PERCENTAGE (15%) of words with special [MASK] symbol\n",
    "        or with random word from vocabulary\n",
    "\n",
    "        Args:\n",
    "            sentence: sentence to process\n",
    "\n",
    "        Returns:\n",
    "            tuple of processed sentence and inverse token mask\n",
    "        \"\"\"\n",
    "        len_s = len(sentence)\n",
    "        inverse_token_mask = [True for _ in range(max(len_s, self.optimal_sentence_length))]\n",
    "\n",
    "        mask_amount = round(len_s * self.MASK_PERCENTAGE)\n",
    "        for _ in range(mask_amount):\n",
    "            i = random.randint(0, len_s - 1)\n",
    "\n",
    "            if random.random() < 0.8:\n",
    "                sentence[i] = self.MASK\n",
    "            else:\n",
    "                # All is below 5 is special token\n",
    "                # see self._insert_specials method\n",
    "                j = random.randint(5, len(self.vocab) - 1)\n",
    "                sentence[i] = self.vocab.lookup_token(j)\n",
    "            inverse_token_mask[i] = False\n",
    "        return sentence, inverse_token_mask    \n",
    "    \n",
    "    def _pad_sentence(self, sentence: typing.List[str], inverse_token_mask: typing.List[bool] = None):\n",
    "        len_s = len(sentence)\n",
    "\n",
    "        if len_s >= self.optimal_sentence_length:\n",
    "            s = sentence[:self.optimal_sentence_length]\n",
    "        else:\n",
    "            s = sentence + [self.PAD] * (self.optimal_sentence_length - len_s)\n",
    "\n",
    "        # inverse token mask should be padded as well\n",
    "        if inverse_token_mask:\n",
    "            len_m = len(inverse_token_mask)\n",
    "            if len_m >= self.optimal_sentence_length:\n",
    "                inverse_token_mask = inverse_token_mask[:self.optimal_sentence_length]\n",
    "            else:\n",
    "                inverse_token_mask = inverse_token_mask + [True] * (self.optimal_sentence_length - len_m)\n",
    "        return s, inverse_token_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 491161/491161 [00:05<00:00, 97674.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [01:31<00:00, 545.23it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          masked_sentence  \\\n",
      "0       [[CLS], one, of, the, other, reviewers, has, m...   \n",
      "1       [[CLS], soon, he, meets, rico, ', ratso, ', ri...   \n",
      "2       [[CLS], they, are, right, ,, as, plasticky, is...   \n",
      "3       [[CLS], the, acting, ., ., ., [MASK], [PAD], [...   \n",
      "4       [[CLS], trust, me, ,, this, is, not, a, [MASK]...   \n",
      "...                                                   ...   \n",
      "882317  [[CLS], the, [MASK], of, them, invite, [MASK],...   \n",
      "882318  [[CLS], no, one, expects, the, star, trek, mov...   \n",
      "882319  [[CLS], i, geezers, ', t, [MASK], you, could, ...   \n",
      "882320  [[CLS], unfortunately, [MASK], this, movie, ha...   \n",
      "882321  [[CLS], another, example, of, what, drags, [MA...   \n",
      "\n",
      "                                           masked_indices  \\\n",
      "0       [0, 5, 6, 7, 8, 9, 10, 11, 2, 13, 14, 15, 2, 2...   \n",
      "1       [0, 1625, 219, 3298, 22768, 20, 24438, 20, 244...   \n",
      "2       [0, 24, 25, 26, 27, 28, 12940, 30, 31, 32, 33,...   \n",
      "3       [0, 7, 468, 36, 36, 36, 2, 1, 1, 1, 1, 1, 1, 1...   \n",
      "4       [0, 54, 35, 27, 29, 30, 55, 56, 2, 58, 7, 59, ...   \n",
      "...                                                   ...   \n",
      "882317  [0, 7, 2, 6, 401, 15689, 2, 352, 1291, 44, 7, ...   \n",
      "882318  [0, 64, 5, 8571, 7, 756, 5005, 574, 67, 22, 10...   \n",
      "882319  [0, 124, 61907, 20, 134, 2, 19, 966, 374, 2646...   \n",
      "882320  [0, 1381, 2, 29, 364, 537, 56, 10578, 27, 2, 2...   \n",
      "882321  [0, 439, 2484, 6, 32, 3496, 2, 1135, 2, 7, 244...   \n",
      "\n",
      "                                                 sentence  \\\n",
      "0       [one, of, the, other, reviewers, has, mentione...   \n",
      "1       [soon, he, meets, rico, ', ratso, ', rizzo, (,...   \n",
      "2       [they, are, right, ,, as, this, is, exactly, w...   \n",
      "3       [the, acting, ., ., ., fair, [SEP], father, na...   \n",
      "4       [trust, me, ,, this, is, not, a, show, for, th...   \n",
      "...                                                   ...   \n",
      "882317  [the, two, of, them, invite, former, friends, ...   \n",
      "882318  [no, one, expects, the, star, trek, movies, to...   \n",
      "882319  [i, don, ', t, think, you, could, make, hercul...   \n",
      "882320  [unfortunately, ,, this, movie, had, a, muddle...   \n",
      "882321  [another, example, of, what, ', s, wrong, with...   \n",
      "\n",
      "                                                  indices  \\\n",
      "0       [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17...   \n",
      "1       [1625, 219, 3298, 22768, 20, 24438, 20, 24439,...   \n",
      "2       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 3...   \n",
      "3       [7, 468, 36, 36, 36, 3166, 3, 1674, 61470, 617...   \n",
      "4       [54, 35, 27, 29, 30, 55, 56, 57, 58, 7, 59, 60...   \n",
      "...                                                   ...   \n",
      "882317  [7, 1661, 6, 401, 15689, 1202, 352, 1291, 44, ...   \n",
      "882318  [64, 5, 8571, 7, 756, 5005, 574, 67, 22, 101, ...   \n",
      "882319  [124, 829, 20, 134, 1081, 19, 966, 374, 2646, ...   \n",
      "882320  [1381, 27, 29, 364, 537, 56, 10578, 27, 2628, ...   \n",
      "882321  [439, 2484, 6, 32, 20, 242, 1135, 34, 7, 2442,...   \n",
      "\n",
      "                                               token_mask  is_next  \n",
      "0       [True, True, True, True, True, True, True, Tru...        1  \n",
      "1       [True, True, True, True, True, True, True, Tru...        0  \n",
      "2       [True, True, True, True, True, True, False, Tr...        1  \n",
      "3       [True, True, True, True, True, True, False, Tr...        0  \n",
      "4       [True, True, True, True, True, True, True, Tru...        1  \n",
      "...                                                   ...      ...  \n",
      "882317  [True, True, False, True, True, True, False, T...        0  \n",
      "882318  [True, True, True, True, True, True, True, Tru...        1  \n",
      "882319  [True, True, False, True, True, False, True, T...        0  \n",
      "882320  [True, True, False, True, True, True, True, Tr...        1  \n",
      "882321  [True, True, True, True, True, False, False, T...        0  \n",
      "\n",
      "[882322 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #BASE_DIR = Path(__file__).resolve().parent.parent\n",
    "\n",
    "    ds = IMDBBertDataset('dataset/IMDB Dataset.csv', ds_from=0, ds_to=50000,\n",
    "                         should_include_text=True)\n",
    "    print(ds.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['masked_sentence', 'masked_indices', 'sentence', 'indices',\n",
       "       'token_mask', 'is_next'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5, 6, 7, 8, 9, 10, 11, 2, 13, 14, 15, 2, 2, 18, 19, 20, 21, 22, 23, 1, 1, 1, 1, 1, 1, 1, 3, 0, 24, 25, 2, 27, 28, 2, 30, 31, 32, 2, 34, 35, 36, 7, 37, 38, 2, 39, 35, 40, 17, 41, 42, 43, 2, 45]\n"
     ]
    }
   ],
   "source": [
    "print(ds.df['masked_indices'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saira\\anaconda3\\envs\\CSCLDL\\lib\\site-packages\\huggingface_hub\\repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"setfit/20_newsgroups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        I was wondering if anyone out there could enli...\n",
       "1        A fair number of brave souls who upgraded thei...\n",
       "2        well folks, my mac plus finally gave up the gh...\n",
       "3        \\nDo you have Weitek's address/phone number?  ...\n",
       "4        From article <C5owCB.n3p@world.std.com>, by to...\n",
       "                               ...                        \n",
       "11309    DN> From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...\n",
       "11310    I have a (very old) Mac 512k and a Mac Plus, b...\n",
       "11311    I just installed a DX2-66 CPU in a clone mothe...\n",
       "11312    \\nWouldn't this require a hyper-sphere.  In 3-...\n",
       "11313    Stolen from Pasadena between 4:30 and 6:30 pm ...\n",
       "Length: 11314, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(dataset['train']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    # Define Special tokens as attributes of class\n",
    "    CLS = '[CLS]'\n",
    "    PAD = '[PAD]'\n",
    "    SEP = '[SEP]'\n",
    "    MASK = '[MASK]'\n",
    "    UNK = '[UNK]'\n",
    "\n",
    "    MASK_PERCENTAGE = 0.15  # How much words to mask\n",
    "\n",
    "    MASKED_INDICES_COLUMN = 'masked_indices'\n",
    "    TARGET_COLUMN = 'indices'\n",
    "    NSP_TARGET_COLUMN = 'is_next'\n",
    "    TOKEN_MASK_COLUMN = 'token_mask'\n",
    "\n",
    "    OPTIMAL_LENGTH_PERCENTILE = 70\n",
    "\n",
    "    def __init__(self, path: str, ds_from=None, ds_to=None, should_include_text=False):\n",
    "        #\"setfit/20_newsgroups\"\n",
    "        self.ds: pd.Series = load_dataset(path)['train']['text']\n",
    "\n",
    "        if ds_from is not None or ds_to is not None:\n",
    "            self.ds = self.ds[ds_from:ds_to]\n",
    "\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        self.counter = Counter()\n",
    "        self.vocab = None\n",
    "\n",
    "        self.optimal_sentence_length = None\n",
    "        self.should_include_text = should_include_text\n",
    "\n",
    "        if should_include_text:\n",
    "            self.columns = ['masked_sentence', self.MASKED_INDICES_COLUMN, 'sentence', self.TARGET_COLUMN,\n",
    "                            self.TOKEN_MASK_COLUMN,\n",
    "                            self.NSP_TARGET_COLUMN]\n",
    "        else:\n",
    "            self.columns = [self.MASKED_INDICES_COLUMN, self.TARGET_COLUMN, self.TOKEN_MASK_COLUMN,\n",
    "                            self.NSP_TARGET_COLUMN]\n",
    "        self.df = pd.DataFrame(self.prepare_dataset())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.iloc[idx]\n",
    "\n",
    "        inp = torch.Tensor(item[self.MASKED_INDICES_COLUMN]).long()\n",
    "        token_mask = torch.Tensor(item[self.TOKEN_MASK_COLUMN]).bool()\n",
    "\n",
    "        mask_target = torch.Tensor(item[self.TARGET_COLUMN]).long()\n",
    "        mask_target = mask_target.masked_fill_(token_mask, 0)\n",
    "\n",
    "        attention_mask = (inp == self.vocab[self.PAD]).unsqueeze(0)\n",
    "\n",
    "        if item[self.NSP_TARGET_COLUMN] == 0:\n",
    "            t = [1, 0]\n",
    "        else:\n",
    "            t = [0, 1]\n",
    "\n",
    "        nsp_target = torch.Tensor(t)\n",
    "\n",
    "        return (\n",
    "            inp.to(device),\n",
    "            attention_mask.to(device),\n",
    "            token_mask.to(device),\n",
    "            mask_target.to(device),\n",
    "            nsp_target.to(device)\n",
    "        )\n",
    "    \n",
    "    def prepare_dataset(self) -> pd.DataFrame:\n",
    "        sentences = []\n",
    "        nsp = []\n",
    "        sentence_lens = []\n",
    "\n",
    "        # Split dataset on sentences\n",
    "        for review in self.ds:\n",
    "            review_sentences = review.split('. ')\n",
    "            sentences += review_sentences\n",
    "            self._update_length(review_sentences, sentence_lens)\n",
    "        self.optimal_sentence_length = self._find_optimal_sentence_length(sentence_lens)\n",
    "\n",
    "        print(\"Create vocabulary\")\n",
    "        for sentence in tqdm(sentences):\n",
    "            s = self.tokenizer(sentence)\n",
    "            self.counter.update(s)\n",
    "\n",
    "        self._fill_vocab()\n",
    "\n",
    "        print(\"Preprocessing dataset\")\n",
    "        for review in tqdm(self.ds):\n",
    "            review_sentences = review.split('. ')\n",
    "            if len(review_sentences) > 1:\n",
    "                for i in range(len(review_sentences) - 1):\n",
    "                    # True NSP item\n",
    "                    first, second = self.tokenizer(review_sentences[i]), self.tokenizer(review_sentences[i + 1])\n",
    "\n",
    "                    nsp.append(self._create_item(first, second, 1))\n",
    "\n",
    "                    # False NSP item\n",
    "                    first, second = self._select_false_nsp_sentences(sentences)\n",
    "                    first, second = self.tokenizer(first), self.tokenizer(second)\n",
    "                    nsp.append(self._create_item(first, second, 0))\n",
    "        df = pd.DataFrame(nsp, columns=self.columns)\n",
    "        return df  \n",
    "    \n",
    "    def _update_length(self, sentences: typing.List[str], lengths: typing.List[int]):\n",
    "        for v in sentences:\n",
    "            l = len(v.split())\n",
    "            lengths.append(l)\n",
    "        return lengths\n",
    "\n",
    "    def _find_optimal_sentence_length(self, lengths: typing.List[int]):\n",
    "        arr = np.array(lengths)\n",
    "        return int(np.percentile(arr, self.OPTIMAL_LENGTH_PERCENTILE))\n",
    "\n",
    "    def _fill_vocab(self):\n",
    "        # specials= argument is only in 0.12.0 version\n",
    "        # specials=[self.CLS, self.PAD, self.MASK, self.SEP, self.UNK]\n",
    "        self.vocab = vocab(self.counter, min_freq=2)\n",
    "\n",
    "        # 0.11.0 uses this approach to insert specials\n",
    "        self.vocab.insert_token(self.CLS, 0)\n",
    "        self.vocab.insert_token(self.PAD, 1)\n",
    "        self.vocab.insert_token(self.MASK, 2)\n",
    "        self.vocab.insert_token(self.SEP, 3)\n",
    "        self.vocab.insert_token(self.UNK, 4)\n",
    "        self.vocab.set_default_index(4)  \n",
    "\n",
    "    def _create_item(self, first: typing.List[str], second: typing.List[str], target: int = 1):\n",
    "        # Create masked sentence item\n",
    "        updated_first, first_mask = self._preprocess_sentence(first.copy())\n",
    "        updated_second, second_mask = self._preprocess_sentence(second.copy())\n",
    "        nsp_sentence = updated_first + [self.SEP] + updated_second\n",
    "        nsp_indices = self.vocab.lookup_indices(nsp_sentence)\n",
    "        inverse_token_mask = first_mask + [True] + second_mask\n",
    "\n",
    "        # Create sentence item without masking random words\n",
    "        first, _ = self._preprocess_sentence(first.copy(), should_mask=False)\n",
    "        second, _ = self._preprocess_sentence(second.copy(), should_mask=False)\n",
    "\n",
    "        original_nsp_sentence = first + [self.SEP] + second\n",
    "        original_nsp_indices = self.vocab.lookup_indices(original_nsp_sentence)\n",
    "\n",
    "        if self.should_include_text:\n",
    "            return (\n",
    "                nsp_sentence,\n",
    "                nsp_indices,\n",
    "                original_nsp_sentence,\n",
    "                original_nsp_indices,\n",
    "                inverse_token_mask,\n",
    "                target\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                nsp_indices,\n",
    "                original_nsp_indices,\n",
    "                inverse_token_mask,\n",
    "                target\n",
    "            )\n",
    "        \n",
    "    def _select_false_nsp_sentences(self, sentences: typing.List[str]):\n",
    "        \"\"\"Select sentences to create false NSP item\n",
    "\n",
    "        Args:\n",
    "            sentences: list of all sentences\n",
    "\n",
    "        Returns:\n",
    "            tuple of two sentences. The second one NOT the next sentence\n",
    "        \"\"\"\n",
    "        sentences_len = len(sentences)\n",
    "        sentence_index = random.randint(0, sentences_len - 1)\n",
    "        next_sentence_index = random.randint(0, sentences_len - 1)\n",
    "\n",
    "        # To be sure that it's not real next sentence\n",
    "        while next_sentence_index == sentence_index + 1:\n",
    "            next_sentence_index = random.randint(0, sentences_len - 1)\n",
    "\n",
    "        return sentences[sentence_index], sentences[next_sentence_index]\n",
    "\n",
    "    def _preprocess_sentence(self, sentence: typing.List[str], should_mask: bool = True):\n",
    "        inverse_token_mask = None\n",
    "        if should_mask == True:\n",
    "            sentence, inverse_token_mask = self._mask_sentence(sentence)\n",
    "            sentence, inverse_token_mask = self._pad_sentence([self.CLS] + sentence, [True] + inverse_token_mask)\n",
    "       \n",
    "        return sentence, inverse_token_mask\n",
    "\n",
    "    def _mask_sentence(self, sentence: typing.List[str]):\n",
    "        \"\"\"Replace MASK_PERCENTAGE (15%) of words with special [MASK] symbol\n",
    "        or with random word from vocabulary\n",
    "\n",
    "        Args:\n",
    "            sentence: sentence to process\n",
    "\n",
    "        Returns:\n",
    "            tuple of processed sentence and inverse token mask\n",
    "        \"\"\"\n",
    "        len_s = len(sentence)\n",
    "        inverse_token_mask = [True for _ in range(max(len_s, self.optimal_sentence_length))]\n",
    "\n",
    "        mask_amount = round(len_s * self.MASK_PERCENTAGE)\n",
    "        for _ in range(mask_amount):\n",
    "            i = random.randint(0, len_s - 1)\n",
    "\n",
    "            if random.random() < 0.8:\n",
    "                sentence[i] = self.MASK\n",
    "            else:\n",
    "                # All is below 5 is special token\n",
    "                # see self._insert_specials method\n",
    "                j = random.randint(5, len(self.vocab) - 1)\n",
    "                sentence[i] = self.vocab.lookup_token(j)\n",
    "            inverse_token_mask[i] = False\n",
    "        return sentence, inverse_token_mask    \n",
    "    \n",
    "    def _pad_sentence(self, sentence: typing.List[str], inverse_token_mask: typing.List[bool] = None):\n",
    "        len_s = len(sentence)\n",
    "\n",
    "        if len_s >= self.optimal_sentence_length:\n",
    "            s = sentence[:self.optimal_sentence_length]\n",
    "        else:\n",
    "            s = sentence + [self.PAD] * (self.optimal_sentence_length - len_s)\n",
    "\n",
    "        # inverse token mask should be padded as well\n",
    "        if inverse_token_mask:\n",
    "            len_m = len(inverse_token_mask)\n",
    "            if len_m >= self.optimal_sentence_length:\n",
    "                inverse_token_mask = inverse_token_mask[:self.optimal_sentence_length]\n",
    "            else:\n",
    "                inverse_token_mask = inverse_token_mask + [True] * (self.optimal_sentence_length - len_m)\n",
    "        return s, inverse_token_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247731/247731 [00:02<00:00, 96381.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:56<00:00, 438.70it/s] \n"
     ]
    }
   ],
   "source": [
    "datas = BertDataset(\"imdb\", should_include_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_sentence    [[CLS], i, rented, [MASK], am, curious-yellow,...\n",
       "masked_indices     [0, 5, 6, 2, 7, 8, 9, 10, 11, 12, 2, 14, 15, 1...\n",
       "sentence           [i, rented, i, am, curious-yellow, from, my, v...\n",
       "indices            [5, 6, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,...\n",
       "token_mask         [True, True, True, False, True, True, True, Tr...\n",
       "is_next                                                            1\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas.df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 335746,\n",
       "         ',': 276280,\n",
       "         'and': 163290,\n",
       "         'a': 162473,\n",
       "         'of': 145437,\n",
       "         'to': 135208,\n",
       "         \"'\": 133857,\n",
       "         'is': 107221,\n",
       "         '.': 104461,\n",
       "         'it': 96024,\n",
       "         'in': 93307,\n",
       "         'i': 87401,\n",
       "         'this': 75878,\n",
       "         'that': 73153,\n",
       "         's': 62933,\n",
       "         'was': 48170,\n",
       "         'as': 46807,\n",
       "         'for': 44116,\n",
       "         'with': 44041,\n",
       "         'movie': 43421,\n",
       "         'but': 42410,\n",
       "         'film': 39459,\n",
       "         ')': 36175,\n",
       "         '(': 35397,\n",
       "         'you': 34141,\n",
       "         't': 33927,\n",
       "         'on': 33740,\n",
       "         'not': 30408,\n",
       "         'he': 30012,\n",
       "         'are': 29406,\n",
       "         'his': 29312,\n",
       "         'have': 27706,\n",
       "         'be': 26743,\n",
       "         'one': 26101,\n",
       "         '!': 24560,\n",
       "         'all': 23522,\n",
       "         'at': 23457,\n",
       "         'they': 22862,\n",
       "         'by': 22414,\n",
       "         'an': 21515,\n",
       "         'who': 21346,\n",
       "         'from': 20474,\n",
       "         'so': 20131,\n",
       "         'like': 19763,\n",
       "         'there': 18769,\n",
       "         'her': 18346,\n",
       "         'or': 17758,\n",
       "         'just': 17717,\n",
       "         'about': 17349,\n",
       "         'if': 16776,\n",
       "         'has': 16759,\n",
       "         'out': 16570,\n",
       "         'what': 16100,\n",
       "         '?': 16088,\n",
       "         'some': 15723,\n",
       "         'good': 14813,\n",
       "         'can': 14626,\n",
       "         'more': 14217,\n",
       "         'when': 14160,\n",
       "         'she': 14136,\n",
       "         'very': 14031,\n",
       "         'even': 12588,\n",
       "         'my': 12469,\n",
       "         'no': 12420,\n",
       "         'up': 12351,\n",
       "         'would': 12342,\n",
       "         'time': 12225,\n",
       "         'which': 11994,\n",
       "         'only': 11874,\n",
       "         'story': 11768,\n",
       "         'really': 11711,\n",
       "         'their': 11375,\n",
       "         'see': 11322,\n",
       "         'had': 11281,\n",
       "         'we': 10835,\n",
       "         'were': 10769,\n",
       "         'me': 10730,\n",
       "         'than': 9847,\n",
       "         'well': 9702,\n",
       "         'much': 9697,\n",
       "         '-': 9687,\n",
       "         'get': 9266,\n",
       "         'been': 9265,\n",
       "         'people': 9251,\n",
       "         'will': 9189,\n",
       "         'also': 9147,\n",
       "         'other': 9141,\n",
       "         'bad': 9131,\n",
       "         'into': 9099,\n",
       "         'do': 9071,\n",
       "         'because': 9033,\n",
       "         'great': 9003,\n",
       "         'first': 8912,\n",
       "         'how': 8875,\n",
       "         'him': 8831,\n",
       "         'don': 8779,\n",
       "         'most': 8763,\n",
       "         'its': 8171,\n",
       "         'made': 8089,\n",
       "         'then': 8069,\n",
       "         'them': 7946,\n",
       "         'way': 7910,\n",
       "         'could': 7906,\n",
       "         'make': 7752,\n",
       "         'too': 7727,\n",
       "         'any': 7649,\n",
       "         'after': 7587,\n",
       "         'movies': 7562,\n",
       "         'think': 7279,\n",
       "         'characters': 7121,\n",
       "         'character': 6935,\n",
       "         'watch': 6912,\n",
       "         'films': 6841,\n",
       "         'two': 6680,\n",
       "         'many': 6667,\n",
       "         'seen': 6648,\n",
       "         'being': 6595,\n",
       "         'never': 6437,\n",
       "         'little': 6406,\n",
       "         'plot': 6403,\n",
       "         'acting': 6402,\n",
       "         'where': 6377,\n",
       "         'best': 6334,\n",
       "         'love': 6320,\n",
       "         'did': 6280,\n",
       "         'life': 6263,\n",
       "         'show': 6216,\n",
       "         'know': 6130,\n",
       "         'does': 5936,\n",
       "         'ever': 5884,\n",
       "         'here': 5735,\n",
       "         'better': 5712,\n",
       "         'your': 5644,\n",
       "         'man': 5640,\n",
       "         'still': 5594,\n",
       "         'over': 5578,\n",
       "         'end': 5552,\n",
       "         'off': 5544,\n",
       "         'these': 5411,\n",
       "         'say': 5383,\n",
       "         'scene': 5305,\n",
       "         'while': 5301,\n",
       "         'why': 5292,\n",
       "         'scenes': 5155,\n",
       "         've': 5127,\n",
       "         'such': 5124,\n",
       "         'go': 5052,\n",
       "         'should': 5027,\n",
       "         'something': 4995,\n",
       "         'through': 4940,\n",
       "         'm': 4920,\n",
       "         'back': 4839,\n",
       "         'those': 4692,\n",
       "         'watching': 4583,\n",
       "         'real': 4558,\n",
       "         'now': 4552,\n",
       "         'though': 4535,\n",
       "         'doesn': 4533,\n",
       "         'thing': 4511,\n",
       "         'years': 4472,\n",
       "         'actors': 4394,\n",
       "         'didn': 4317,\n",
       "         'another': 4313,\n",
       "         'before': 4299,\n",
       "         'nothing': 4271,\n",
       "         'new': 4248,\n",
       "         'actually': 4236,\n",
       "         'funny': 4223,\n",
       "         'work': 4217,\n",
       "         'makes': 4193,\n",
       "         'find': 4126,\n",
       "         'director': 4103,\n",
       "         'look': 4089,\n",
       "         'few': 4071,\n",
       "         'old': 4071,\n",
       "         'going': 4037,\n",
       "         'same': 4031,\n",
       "         'lot': 3971,\n",
       "         'every': 3965,\n",
       "         'again': 3965,\n",
       "         'part': 3964,\n",
       "         're': 3774,\n",
       "         'cast': 3762,\n",
       "         'us': 3754,\n",
       "         'world': 3725,\n",
       "         'quite': 3721,\n",
       "         'want': 3693,\n",
       "         'things': 3670,\n",
       "         'pretty': 3652,\n",
       "         'young': 3641,\n",
       "         'seems': 3617,\n",
       "         'got': 3586,\n",
       "         'around': 3583,\n",
       "         'down': 3554,\n",
       "         'however': 3529,\n",
       "         '&': 3507,\n",
       "         'fact': 3506,\n",
       "         'take': 3473,\n",
       "         'enough': 3436,\n",
       "         'both': 3400,\n",
       "         'give': 3371,\n",
       "         'between': 3367,\n",
       "         'may': 3364,\n",
       "         'original': 3362,\n",
       "         'horror': 3357,\n",
       "         'big': 3345,\n",
       "         'own': 3339,\n",
       "         'thought': 3318,\n",
       "         'series': 3300,\n",
       "         'without': 3254,\n",
       "         'right': 3240,\n",
       "         'times': 3222,\n",
       "         'long': 3220,\n",
       "         'always': 3216,\n",
       "         'gets': 3200,\n",
       "         'isn': 3173,\n",
       "         'come': 3167,\n",
       "         'saw': 3155,\n",
       "         'role': 3155,\n",
       "         'family': 3153,\n",
       "         'point': 3147,\n",
       "         'almost': 3117,\n",
       "         'interesting': 3115,\n",
       "         'must': 3106,\n",
       "         'least': 3106,\n",
       "         'action': 3106,\n",
       "         'comedy': 3099,\n",
       "         'whole': 3066,\n",
       "         'bit': 3029,\n",
       "         'music': 3004,\n",
       "         'done': 3001,\n",
       "         'script': 2989,\n",
       "         'guy': 2971,\n",
       "         'minutes': 2935,\n",
       "         'anything': 2932,\n",
       "         'last': 2916,\n",
       "         'might': 2909,\n",
       "         'since': 2897,\n",
       "         'll': 2888,\n",
       "         'performance': 2887,\n",
       "         'far': 2877,\n",
       "         'feel': 2876,\n",
       "         'probably': 2832,\n",
       "         'd': 2811,\n",
       "         'am': 2773,\n",
       "         'kind': 2754,\n",
       "         'woman': 2752,\n",
       "         'away': 2743,\n",
       "         'girl': 2734,\n",
       "         'rather': 2730,\n",
       "         'yet': 2725,\n",
       "         'worst': 2703,\n",
       "         'sure': 2672,\n",
       "         'fun': 2645,\n",
       "         'anyone': 2630,\n",
       "         'making': 2602,\n",
       "         'each': 2578,\n",
       "         'played': 2569,\n",
       "         'found': 2566,\n",
       "         'tv': 2552,\n",
       "         'having': 2538,\n",
       "         'day': 2533,\n",
       "         'although': 2523,\n",
       "         'our': 2505,\n",
       "         'especially': 2499,\n",
       "         'course': 2494,\n",
       "         'believe': 2492,\n",
       "         'comes': 2483,\n",
       "         'trying': 2468,\n",
       "         'goes': 2434,\n",
       "         'looks': 2408,\n",
       "         'hard': 2407,\n",
       "         'place': 2380,\n",
       "         'different': 2380,\n",
       "         'book': 2357,\n",
       "         'put': 2348,\n",
       "         'someone': 2334,\n",
       "         'maybe': 2328,\n",
       "         'money': 2327,\n",
       "         'ending': 2320,\n",
       "         'reason': 2314,\n",
       "         'wasn': 2307,\n",
       "         'let': 2307,\n",
       "         'actor': 2306,\n",
       "         'sense': 2305,\n",
       "         'everything': 2301,\n",
       "         'once': 2301,\n",
       "         'shows': 2297,\n",
       "         'screen': 2288,\n",
       "         'true': 2282,\n",
       "         '2': 2280,\n",
       "         'set': 2278,\n",
       "         'dvd': 2267,\n",
       "         'worth': 2264,\n",
       "         'looking': 2260,\n",
       "         'job': 2257,\n",
       "         'main': 2256,\n",
       "         'watched': 2226,\n",
       "         'everyone': 2216,\n",
       "         'together': 2216,\n",
       "         'plays': 2208,\n",
       "         'three': 2204,\n",
       "         'john': 2199,\n",
       "         'said': 2190,\n",
       "         'later': 2187,\n",
       "         'play': 2184,\n",
       "         'instead': 2183,\n",
       "         'audience': 2179,\n",
       "         'seem': 2174,\n",
       "         '10': 2172,\n",
       "         'takes': 2165,\n",
       "         'beautiful': 2158,\n",
       "         'effects': 2158,\n",
       "         'himself': 2152,\n",
       "         'version': 2140,\n",
       "         'during': 2129,\n",
       "         'left': 2090,\n",
       "         'seeing': 2084,\n",
       "         'night': 2083,\n",
       "         'special': 2077,\n",
       "         'house': 2073,\n",
       "         'excellent': 2065,\n",
       "         'wife': 2063,\n",
       "         'american': 2038,\n",
       "         'idea': 2037,\n",
       "         'father': 2029,\n",
       "         'else': 1999,\n",
       "         'nice': 1991,\n",
       "         'shot': 1987,\n",
       "         'simply': 1966,\n",
       "         'year': 1954,\n",
       "         'read': 1927,\n",
       "         'high': 1909,\n",
       "         'black': 1907,\n",
       "         'less': 1904,\n",
       "         'war': 1896,\n",
       "         'star': 1891,\n",
       "         'help': 1889,\n",
       "         'completely': 1887,\n",
       "         'poor': 1879,\n",
       "         'fan': 1878,\n",
       "         'second': 1878,\n",
       "         'death': 1873,\n",
       "         'hollywood': 1859,\n",
       "         'either': 1856,\n",
       "         'men': 1850,\n",
       "         'mind': 1847,\n",
       "         'used': 1846,\n",
       "         'given': 1844,\n",
       "         'home': 1826,\n",
       "         'kids': 1824,\n",
       "         'try': 1823,\n",
       "         'performances': 1816,\n",
       "         'enjoy': 1811,\n",
       "         'classic': 1809,\n",
       "         'boring': 1804,\n",
       "         'rest': 1800,\n",
       "         'short': 1799,\n",
       "         'need': 1799,\n",
       "         'women': 1798,\n",
       "         'wrong': 1798,\n",
       "         'use': 1792,\n",
       "         'until': 1770,\n",
       "         'friends': 1765,\n",
       "         'along': 1765,\n",
       "         'dead': 1758,\n",
       "         'truly': 1742,\n",
       "         'half': 1732,\n",
       "         'production': 1730,\n",
       "         'line': 1725,\n",
       "         'tell': 1706,\n",
       "         'couple': 1703,\n",
       "         'remember': 1700,\n",
       "         'next': 1696,\n",
       "         '--': 1691,\n",
       "         'start': 1685,\n",
       "         'stupid': 1678,\n",
       "         'won': 1675,\n",
       "         'perhaps': 1673,\n",
       "         'came': 1672,\n",
       "         'recommend': 1668,\n",
       "         'awful': 1659,\n",
       "         'moments': 1655,\n",
       "         'wonderful': 1651,\n",
       "         'episode': 1648,\n",
       "         'mean': 1645,\n",
       "         'understand': 1643,\n",
       "         'full': 1633,\n",
       "         'terrible': 1631,\n",
       "         'getting': 1619,\n",
       "         'camera': 1619,\n",
       "         'playing': 1608,\n",
       "         'stars': 1607,\n",
       "         'keep': 1599,\n",
       "         'doing': 1591,\n",
       "         'others': 1591,\n",
       "         'video': 1584,\n",
       "         'often': 1583,\n",
       "         'small': 1582,\n",
       "         'sex': 1579,\n",
       "         'definitely': 1579,\n",
       "         'gives': 1576,\n",
       "         'school': 1563,\n",
       "         'person': 1563,\n",
       "         'perfect': 1563,\n",
       "         'early': 1560,\n",
       "         'face': 1558,\n",
       "         'itself': 1554,\n",
       "         'name': 1551,\n",
       "         'become': 1541,\n",
       "         'human': 1534,\n",
       "         'finally': 1532,\n",
       "         'lines': 1531,\n",
       "         'dialogue': 1528,\n",
       "         'yes': 1524,\n",
       "         'felt': 1522,\n",
       "         'case': 1521,\n",
       "         'lost': 1517,\n",
       "         'piece': 1514,\n",
       "         'supposed': 1510,\n",
       "         'liked': 1510,\n",
       "         'top': 1503,\n",
       "         'children': 1499,\n",
       "         'couldn': 1492,\n",
       "         'absolutely': 1484,\n",
       "         'head': 1479,\n",
       "         'live': 1477,\n",
       "         'title': 1475,\n",
       "         'written': 1472,\n",
       "         'budget': 1471,\n",
       "         'mother': 1469,\n",
       "         'against': 1468,\n",
       "         'picture': 1466,\n",
       "         'boy': 1464,\n",
       "         'cinema': 1462,\n",
       "         'certainly': 1462,\n",
       "         'went': 1462,\n",
       "         'entire': 1459,\n",
       "         'style': 1457,\n",
       "         'sort': 1456,\n",
       "         'worse': 1456,\n",
       "         '3': 1455,\n",
       "         'waste': 1452,\n",
       "         'hope': 1443,\n",
       "         'problem': 1442,\n",
       "         'mr': 1436,\n",
       "         'entertaining': 1429,\n",
       "         'overall': 1428,\n",
       "         'evil': 1427,\n",
       "         'killer': 1419,\n",
       "         'several': 1418,\n",
       "         'friend': 1417,\n",
       "         '1': 1416,\n",
       "         'loved': 1410,\n",
       "         'fans': 1407,\n",
       "         'beginning': 1401,\n",
       "         'oh': 1397,\n",
       "         'lives': 1383,\n",
       "         'becomes': 1378,\n",
       "         'white': 1376,\n",
       "         'care': 1374,\n",
       "         'direction': 1372,\n",
       "         'already': 1371,\n",
       "         'example': 1368,\n",
       "         'based': 1366,\n",
       "         'seemed': 1362,\n",
       "         'despite': 1362,\n",
       "         'dark': 1358,\n",
       "         'throughout': 1356,\n",
       "         'unfortunately': 1352,\n",
       "         'wanted': 1349,\n",
       "         '\\x96': 1337,\n",
       "         'drama': 1324,\n",
       "         'final': 1322,\n",
       "         'amazing': 1319,\n",
       "         'turn': 1318,\n",
       "         'history': 1318,\n",
       "         'fine': 1316,\n",
       "         'michael': 1314,\n",
       "         'laugh': 1313,\n",
       "         'guess': 1306,\n",
       "         'totally': 1305,\n",
       "         'humor': 1298,\n",
       "         'son': 1295,\n",
       "         'lead': 1293,\n",
       "         'guys': 1291,\n",
       "         'sound': 1289,\n",
       "         'wants': 1286,\n",
       "         'writing': 1282,\n",
       "         'low': 1275,\n",
       "         'works': 1275,\n",
       "         'tries': 1273,\n",
       "         'called': 1267,\n",
       "         'viewer': 1260,\n",
       "         'under': 1260,\n",
       "         'past': 1259,\n",
       "         'quality': 1256,\n",
       "         'days': 1254,\n",
       "         'behind': 1253,\n",
       "         'game': 1251,\n",
       "         'enjoyed': 1246,\n",
       "         'turns': 1246,\n",
       "         'child': 1246,\n",
       "         'today': 1243,\n",
       "         'able': 1240,\n",
       "         'act': 1230,\n",
       "         'favorite': 1225,\n",
       "         'town': 1225,\n",
       "         'kill': 1223,\n",
       "         'flick': 1220,\n",
       "         'starts': 1218,\n",
       "         'gave': 1217,\n",
       "         'sometimes': 1214,\n",
       "         'eyes': 1206,\n",
       "         'side': 1200,\n",
       "         'soon': 1197,\n",
       "         'girls': 1197,\n",
       "         'etc': 1196,\n",
       "         'horrible': 1193,\n",
       "         'car': 1193,\n",
       "         'actress': 1189,\n",
       "         'genre': 1189,\n",
       "         'brilliant': 1189,\n",
       "         'parts': 1185,\n",
       "         'art': 1182,\n",
       "         'heart': 1181,\n",
       "         'themselves': 1179,\n",
       "         'expect': 1175,\n",
       "         'kid': 1168,\n",
       "         'stuff': 1164,\n",
       "         'stories': 1161,\n",
       "         'thinking': 1160,\n",
       "         'city': 1160,\n",
       "         'obviously': 1159,\n",
       "         'directed': 1150,\n",
       "         'myself': 1146,\n",
       "         'late': 1142,\n",
       "         'blood': 1140,\n",
       "         'decent': 1139,\n",
       "         'run': 1137,\n",
       "         'feeling': 1137,\n",
       "         'highly': 1131,\n",
       "         'close': 1124,\n",
       "         'fight': 1123,\n",
       "         'god': 1123,\n",
       "         'except': 1122,\n",
       "         'hand': 1118,\n",
       "         'anyway': 1114,\n",
       "         'roles': 1109,\n",
       "         'moment': 1107,\n",
       "         'says': 1106,\n",
       "         'heard': 1105,\n",
       "         'killed': 1105,\n",
       "         'leave': 1100,\n",
       "         'matter': 1099,\n",
       "         'took': 1099,\n",
       "         'daughter': 1097,\n",
       "         'cannot': 1096,\n",
       "         'police': 1080,\n",
       "         'happens': 1079,\n",
       "         'happened': 1074,\n",
       "         'hour': 1072,\n",
       "         'strong': 1071,\n",
       "         'brother': 1071,\n",
       "         'involved': 1066,\n",
       "         'james': 1065,\n",
       "         'extremely': 1064,\n",
       "         'chance': 1061,\n",
       "         'violence': 1061,\n",
       "         'particularly': 1058,\n",
       "         'obvious': 1055,\n",
       "         'wouldn': 1054,\n",
       "         '5': 1054,\n",
       "         'experience': 1053,\n",
       "         'lack': 1052,\n",
       "         'attempt': 1049,\n",
       "         'told': 1046,\n",
       "         'living': 1045,\n",
       "         'alone': 1045,\n",
       "         'please': 1040,\n",
       "         'age': 1040,\n",
       "         'happen': 1039,\n",
       "         'murder': 1035,\n",
       "         'wonder': 1034,\n",
       "         'including': 1033,\n",
       "         'ago': 1027,\n",
       "         'complete': 1027,\n",
       "         'group': 1026,\n",
       "         'voice': 1024,\n",
       "         'david': 1021,\n",
       "         'coming': 1021,\n",
       "         'score': 1021,\n",
       "         'save': 1019,\n",
       "         'interest': 1014,\n",
       "         'none': 1010,\n",
       "         'type': 1006,\n",
       "         'looked': 1006,\n",
       "         'ok': 1006,\n",
       "         '4': 1005,\n",
       "         'simple': 1004,\n",
       "         'crap': 1004,\n",
       "         'number': 1000,\n",
       "         'slow': 999,\n",
       "         'hell': 998,\n",
       "         'exactly': 995,\n",
       "         'shown': 992,\n",
       "         'seriously': 992,\n",
       "         'annoying': 991,\n",
       "         'husband': 985,\n",
       "         'taken': 984,\n",
       "         'whose': 984,\n",
       "         'yourself': 984,\n",
       "         'king': 982,\n",
       "         'career': 982,\n",
       "         'song': 981,\n",
       "         'sad': 981,\n",
       "         'usually': 978,\n",
       "         'cinematography': 978,\n",
       "         'serious': 977,\n",
       "         'stop': 976,\n",
       "         'scary': 975,\n",
       "         'possible': 975,\n",
       "         'ends': 974,\n",
       "         'hours': 973,\n",
       "         'hero': 973,\n",
       "         'gore': 970,\n",
       "         'released': 968,\n",
       "         'across': 968,\n",
       "         'musical': 967,\n",
       "         'running': 965,\n",
       "         'hilarious': 965,\n",
       "         'usual': 962,\n",
       "         'reality': 961,\n",
       "         'opening': 961,\n",
       "         'somewhat': 960,\n",
       "         'ridiculous': 959,\n",
       "         'known': 959,\n",
       "         'relationship': 959,\n",
       "         'started': 956,\n",
       "         'opinion': 956,\n",
       "         'hit': 954,\n",
       "         'novel': 952,\n",
       "         'jokes': 951,\n",
       "         'wish': 950,\n",
       "         'cool': 949,\n",
       "         'change': 949,\n",
       "         'robert': 947,\n",
       "         'finds': 947,\n",
       "         'ones': 946,\n",
       "         'huge': 942,\n",
       "         'saying': 942,\n",
       "         'order': 942,\n",
       "         'shots': 941,\n",
       "         'body': 939,\n",
       "         'episodes': 936,\n",
       "         'english': 936,\n",
       "         'cut': 936,\n",
       "         'mostly': 933,\n",
       "         'taking': 929,\n",
       "         'female': 925,\n",
       "         'talking': 923,\n",
       "         'major': 919,\n",
       "         'view': 917,\n",
       "         'strange': 916,\n",
       "         'disappointed': 915,\n",
       "         'power': 915,\n",
       "         'level': 914,\n",
       "         'documentary': 912,\n",
       "         'talent': 912,\n",
       "         'call': 911,\n",
       "         'apparently': 911,\n",
       "         'happy': 911,\n",
       "         'due': 907,\n",
       "         'rating': 907,\n",
       "         'room': 905,\n",
       "         'events': 905,\n",
       "         'important': 904,\n",
       "         'jack': 904,\n",
       "         'songs': 903,\n",
       "         'basically': 903,\n",
       "         'country': 902,\n",
       "         'clearly': 899,\n",
       "         'knew': 897,\n",
       "         'knows': 897,\n",
       "         'supporting': 897,\n",
       "         'easily': 888,\n",
       "         'turned': 888,\n",
       "         'future': 887,\n",
       "         'attention': 887,\n",
       "         'television': 885,\n",
       "         'paul': 884,\n",
       "         'aren': 883,\n",
       "         'silly': 883,\n",
       "         'word': 882,\n",
       "         'british': 882,\n",
       "         'problems': 880,\n",
       "         'earth': 880,\n",
       "         'tells': 879,\n",
       "         'single': 878,\n",
       "         'local': 876,\n",
       "         'words': 874,\n",
       "         'bring': 869,\n",
       "         'sequence': 867,\n",
       "         'cheap': 866,\n",
       "         'four': 864,\n",
       "         'light': 864,\n",
       "         'entertainment': 864,\n",
       "         'beyond': 862,\n",
       "         'miss': 858,\n",
       "         'george': 857,\n",
       "         'whether': 855,\n",
       "         'modern': 854,\n",
       "         'predictable': 852,\n",
       "         'five': 851,\n",
       "         'falls': 850,\n",
       "         'sets': 849,\n",
       "         'similar': 846,\n",
       "         'richard': 845,\n",
       "         'review': 844,\n",
       "         'writer': 840,\n",
       "         'needs': 839,\n",
       "         'appears': 838,\n",
       "         'upon': 838,\n",
       "         'enjoyable': 838,\n",
       "         'lady': 836,\n",
       "         'romantic': 835,\n",
       "         'rock': 832,\n",
       "         'giving': 830,\n",
       "         'comic': 830,\n",
       "         'talk': 823,\n",
       "         'message': 822,\n",
       "         'within': 820,\n",
       "         'theater': 819,\n",
       "         'animation': 817,\n",
       "         'bunch': 812,\n",
       "         'mention': 811,\n",
       "         'feels': 810,\n",
       "         'nearly': 809,\n",
       "         'sequel': 809,\n",
       "         'haven': 807,\n",
       "         'points': 807,\n",
       "         'dull': 805,\n",
       "         'above': 804,\n",
       "         'add': 804,\n",
       "         'lee': 803,\n",
       "         'moving': 803,\n",
       "         'york': 802,\n",
       "         'ways': 802,\n",
       "         'surprised': 801,\n",
       "         'herself': 799,\n",
       "         'theme': 799,\n",
       "         'storyline': 798,\n",
       "         'lots': 797,\n",
       "         'ten': 797,\n",
       "         'team': 796,\n",
       "         'mystery': 795,\n",
       "         'using': 795,\n",
       "         'begins': 795,\n",
       "         'actual': 793,\n",
       "         'middle': 792,\n",
       "         'thriller': 791,\n",
       "         'fantastic': 791,\n",
       "         'effort': 789,\n",
       "         'sister': 787,\n",
       "         'named': 785,\n",
       "         'viewers': 783,\n",
       "         'easy': 782,\n",
       "         'elements': 779,\n",
       "         'among': 779,\n",
       "         'stay': 779,\n",
       "         'tom': 777,\n",
       "         'avoid': 776,\n",
       "         'typical': 775,\n",
       "         'comments': 775,\n",
       "         'showing': 774,\n",
       "         'release': 771,\n",
       "         'clear': 771,\n",
       "         'peter': 770,\n",
       "         'tried': 769,\n",
       "         'certain': 765,\n",
       "         'sorry': 764,\n",
       "         'season': 763,\n",
       "         'tale': 763,\n",
       "         'french': 762,\n",
       "         'dialog': 762,\n",
       "         'fall': 761,\n",
       "         'soundtrack': 760,\n",
       "         'famous': 760,\n",
       "         'straight': 760,\n",
       "         'general': 759,\n",
       "         'means': 757,\n",
       "         'near': 757,\n",
       "         'check': 757,\n",
       "         'somehow': 757,\n",
       "         'editing': 755,\n",
       "         'material': 755,\n",
       "         'form': 755,\n",
       "         'hate': 755,\n",
       "         'red': 754,\n",
       "         'parents': 753,\n",
       "         'doubt': 753,\n",
       "         'buy': 752,\n",
       "         'oscar': 752,\n",
       "         'class': 750,\n",
       "         'period': 749,\n",
       "         'working': 749,\n",
       "         'filmed': 747,\n",
       "         'weak': 747,\n",
       "         'gone': 747,\n",
       "         'kept': 747,\n",
       "         'leads': 747,\n",
       "         'greatest': 745,\n",
       "         'figure': 743,\n",
       "         'o': 743,\n",
       "         'viewing': 740,\n",
       "         'feature': 738,\n",
       "         'brought': 737,\n",
       "         'eye': 737,\n",
       "         'realistic': 736,\n",
       "         'disney': 736,\n",
       "         'imagine': 735,\n",
       "         'hear': 733,\n",
       "         'atmosphere': 730,\n",
       "         'particular': 727,\n",
       "         'fast': 727,\n",
       "         'move': 724,\n",
       "         'lame': 722,\n",
       "         'indeed': 720,\n",
       "         'sequences': 719,\n",
       "         'eventually': 719,\n",
       "         'learn': 718,\n",
       "         'die': 718,\n",
       "         'follow': 718,\n",
       "         'america': 717,\n",
       "         'wait': 714,\n",
       "         'reviews': 714,\n",
       "         'deal': 714,\n",
       "         'forget': 714,\n",
       "         'space': 710,\n",
       "         'dance': 709,\n",
       "         'suspense': 709,\n",
       "         'whatever': 708,\n",
       "         'believable': 708,\n",
       "         'zombie': 708,\n",
       "         'okay': 708,\n",
       "         'premise': 707,\n",
       "         'b': 706,\n",
       "         'surprise': 705,\n",
       "         'crime': 705,\n",
       "         'decided': 704,\n",
       "         'third': 703,\n",
       "         'subject': 701,\n",
       "         'expected': 701,\n",
       "         'possibly': 701,\n",
       "         'nature': 700,\n",
       "         'de': 699,\n",
       "         'became': 697,\n",
       "         'truth': 695,\n",
       "         'stand': 695,\n",
       "         'japanese': 695,\n",
       "         'dr': 695,\n",
       "         'average': 694,\n",
       "         'difficult': 693,\n",
       "         'sexual': 690,\n",
       "         'screenplay': 689,\n",
       "         'sit': 689,\n",
       "         'imdb': 689,\n",
       "         'joe': 688,\n",
       "         'poorly': 688,\n",
       "         'rent': 686,\n",
       "         'nor': 684,\n",
       "         'leaves': 682,\n",
       "         'stage': 681,\n",
       "         'romance': 681,\n",
       "         'question': 679,\n",
       "         'begin': 676,\n",
       "         'killing': 676,\n",
       "         'reading': 676,\n",
       "         'needed': 676,\n",
       "         'unless': 670,\n",
       "         'baby': 669,\n",
       "         'street': 669,\n",
       "         'shame': 668,\n",
       "         'note': 668,\n",
       "         'otherwise': 667,\n",
       "         'society': 666,\n",
       "         'situation': 666,\n",
       "         'meet': 666,\n",
       "         'superb': 666,\n",
       "         'meets': 664,\n",
       "         'memorable': 664,\n",
       "         'forced': 660,\n",
       "         'earlier': 659,\n",
       "         'credits': 659,\n",
       "         'dog': 659,\n",
       "         'directors': 657,\n",
       "         'minute': 655,\n",
       "         'weird': 655,\n",
       "         'emotional': 653,\n",
       "         'realize': 652,\n",
       "         'beauty': 651,\n",
       "         'older': 651,\n",
       "         'jane': 651,\n",
       "         'comment': 649,\n",
       "         'footage': 648,\n",
       "         'ask': 647,\n",
       "         'interested': 647,\n",
       "         'write': 646,\n",
       "         'laughs': 646,\n",
       "         'badly': 645,\n",
       "         'sounds': 642,\n",
       "         'whom': 642,\n",
       "         'keeps': 642,\n",
       "         'features': 640,\n",
       "         'dramatic': 640,\n",
       "         'development': 638,\n",
       "         'mess': 636,\n",
       "         'hot': 636,\n",
       "         'towards': 636,\n",
       "         'quickly': 634,\n",
       "         'writers': 634,\n",
       "         'free': 632,\n",
       "         'crazy': 632,\n",
       "         'male': 631,\n",
       "         'total': 631,\n",
       "         'mark': 631,\n",
       "         'previous': 630,\n",
       "         'directing': 630,\n",
       "         'result': 630,\n",
       "         'brings': 630,\n",
       "         'perfectly': 630,\n",
       "         'creepy': 629,\n",
       "         'unique': 628,\n",
       "         'plus': 627,\n",
       "         'worked': 627,\n",
       "         'plenty': 627,\n",
       "         'effect': 626,\n",
       "         'personal': 625,\n",
       "         'cheesy': 625,\n",
       "         'incredibly': 625,\n",
       "         'bill': 624,\n",
       "         'hands': 623,\n",
       "         'monster': 623,\n",
       "         'return': 621,\n",
       "         'deep': 621,\n",
       "         'apart': 620,\n",
       "         'admit': 620,\n",
       "         'setting': 618,\n",
       "         'dream': 617,\n",
       "         'appear': 616,\n",
       "         'background': 613,\n",
       "         'open': 613,\n",
       "         'casting': 612,\n",
       "         'leading': 612,\n",
       "         'hardly': 611,\n",
       "         'meant': 611,\n",
       "         'christmas': 611,\n",
       "         'potential': 610,\n",
       "         'ben': 609,\n",
       "         'powerful': 609,\n",
       "         'boys': 608,\n",
       "         'business': 608,\n",
       "         'doctor': 607,\n",
       "         'fails': 606,\n",
       "         'various': 604,\n",
       "         'masterpiece': 603,\n",
       "         'create': 603,\n",
       "         'joke': 602,\n",
       "         '7': 601,\n",
       "         'battle': 601,\n",
       "         'forward': 600,\n",
       "         'fire': 599,\n",
       "         'fantasy': 598,\n",
       "         'outside': 597,\n",
       "         'portrayed': 596,\n",
       "         'inside': 596,\n",
       "         'william': 595,\n",
       "         '20': 593,\n",
       "         'twist': 593,\n",
       "         'secret': 593,\n",
       "         'ideas': 593,\n",
       "         'missing': 592,\n",
       "         'reasons': 592,\n",
       "         'deserves': 591,\n",
       "         'dumb': 588,\n",
       "         'villain': 587,\n",
       "         'fairly': 586,\n",
       "         'air': 586,\n",
       "         'expecting': 586,\n",
       "         'present': 585,\n",
       "         'fighting': 585,\n",
       "         'girlfriend': 584,\n",
       "         'manages': 583,\n",
       "         'attempts': 583,\n",
       "         'unlike': 583,\n",
       "         'married': 582,\n",
       "         'nudity': 582,\n",
       "         'gay': 582,\n",
       "         'break': 582,\n",
       "         'political': 581,\n",
       "         ...})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas.counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,     5,     6,     2,     7,     8,     9,    10,    11,    12,\n",
      "            2,    14,    15,    16,    17,    18,    19,    20,    21,     2,\n",
      "           22,    23,    24,    25,    26,     1,     1,     3,     0,     2,\n",
      "           27,    28,    18, 32891,    23,    20,    22,    30,    31,    32,\n",
      "           33,    34,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1])\n",
      "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False,  True,  True, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "item = datas.df.iloc[0]\n",
    "inp = torch.Tensor(item['masked_indices']).long()\n",
    "token_mask = torch.Tensor(item['token_mask']).bool()\n",
    "print(inp)\n",
    "attention_mask = (inp == datas.vocab[datas.PAD]).unsqueeze(0)\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([38])\n",
      "tensor([ True,  True,  True, False,  True,  True,  True,  True,  True,  True,\n",
      "        False,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "         True,  True,  True, False,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (38) must match the size of tensor b (55) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(mask_target\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(token_mask)\n\u001b[1;32m----> 4\u001b[0m mask_target \u001b[38;5;241m=\u001b[39m \u001b[43mmask_target\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(mask_target)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (38) must match the size of tensor b (55) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "mask_target = torch.Tensor(item[datas.TARGET_COLUMN]).long()  \n",
    "print(mask_target.size())\n",
    "print(token_mask)\n",
    "mask_target = mask_target.masked_fill_(token_mask, 0)\n",
    "print(mask_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 7.81k/7.81k [00:00<?, ?B/s]\n",
      "Downloading data: 100%|██████████| 21.0M/21.0M [00:01<00:00, 18.1MB/s]\n",
      "Downloading data: 100%|██████████| 20.5M/20.5M [00:02<00:00, 10.1MB/s]\n",
      "Downloading data: 100%|██████████| 42.0M/42.0M [00:02<00:00, 18.6MB/s]\n",
      "Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 227645.96 examples/s]\n",
      "Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 351766.08 examples/s]\n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 367610.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<?, ?B/s]\n",
      "c:\\Users\\saira\\anaconda3\\envs\\CSCLDL\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\saira\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "config.json: 100%|██████████| 483/483 [00:00<00:00, 319kB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 37.5MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 2.14MB/s]\n",
      "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 25000/25000 [00:04<00:00, 6204.67 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:04<00:00, 5789.65 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [00:09<00:00, 5239.65 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_imdb = imdb.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_imdb['train']['attention_mask'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCLDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
