{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0weV-qC0OZh",
        "outputId": "f304c34d-0d5d-414b-a078-c58c772b294f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
            "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import re\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from avalanche.benchmarks import nc_benchmark\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"SetFit/20_newsgroups\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQ_uZqZP0OZj",
        "outputId": "d7a7a559-c5e9-4992-9956-4e09da5462af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label', 'label_text'],\n",
              "        num_rows: 11314\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label', 'label_text'],\n",
              "        num_rows: 7532\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4eXXS2o0OZk",
        "outputId": "8f9b2a2b-dd8b-40ed-f0f3-976d1110182b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Label Distribution:\n",
            "Label 0: 480 samples\n",
            "Label 1: 584 samples\n",
            "Label 2: 591 samples\n",
            "Label 3: 590 samples\n",
            "Label 4: 578 samples\n",
            "Label 5: 593 samples\n",
            "Label 6: 585 samples\n",
            "Label 7: 594 samples\n",
            "Label 8: 598 samples\n",
            "Label 9: 597 samples\n",
            "Label 10: 600 samples\n",
            "Label 11: 595 samples\n",
            "Label 12: 591 samples\n",
            "Label 13: 594 samples\n",
            "Label 14: 593 samples\n",
            "Label 15: 599 samples\n",
            "Label 16: 546 samples\n",
            "Label 17: 564 samples\n",
            "Label 18: 465 samples\n",
            "Label 19: 377 samples\n",
            "\n",
            "Test Label Distribution:\n",
            "Label 0: 319 samples\n",
            "Label 1: 389 samples\n",
            "Label 2: 394 samples\n",
            "Label 3: 392 samples\n",
            "Label 4: 385 samples\n",
            "Label 5: 395 samples\n",
            "Label 6: 390 samples\n",
            "Label 7: 396 samples\n",
            "Label 8: 398 samples\n",
            "Label 9: 397 samples\n",
            "Label 10: 399 samples\n",
            "Label 11: 396 samples\n",
            "Label 12: 393 samples\n",
            "Label 13: 396 samples\n",
            "Label 14: 394 samples\n",
            "Label 15: 398 samples\n",
            "Label 16: 364 samples\n",
            "Label 17: 376 samples\n",
            "Label 18: 310 samples\n",
            "Label 19: 251 samples\n"
          ]
        }
      ],
      "source": [
        "# Assuming dataset is a DatasetDict containing train and test datasets\n",
        "# Iterate over the train dataset to count label occurrences\n",
        "train_label_counts = {}\n",
        "for example in dataset['train']:\n",
        "    label = example['label']\n",
        "    if label in train_label_counts:\n",
        "        train_label_counts[label] += 1\n",
        "    else:\n",
        "        train_label_counts[label] = 1\n",
        "\n",
        "# Iterate over the test dataset to count label occurrences\n",
        "test_label_counts = {}\n",
        "for example in dataset['test']:\n",
        "    label = example['label']\n",
        "    if label in test_label_counts:\n",
        "        test_label_counts[label] += 1\n",
        "    else:\n",
        "        test_label_counts[label] = 1\n",
        "\n",
        "# Sort train label counts by label number\n",
        "sorted_train_label_counts = sorted(train_label_counts.items(), key=lambda x: x[0])\n",
        "\n",
        "# Print sorted train label distributions\n",
        "print(\"Train Label Distribution:\")\n",
        "for label, count in sorted_train_label_counts:\n",
        "    print(f\"Label {label}: {count} samples\")\n",
        "\n",
        "# Sort test label counts by label number\n",
        "sorted_test_label_counts = sorted(test_label_counts.items(), key=lambda x: x[0])\n",
        "\n",
        "# Print sorted test label distributions\n",
        "print(\"\\nTest Label Distribution:\")\n",
        "for label, count in sorted_test_label_counts:\n",
        "    print(f\"Label {label}: {count} samples\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IwaoSPD0OZk",
        "outputId": "3eea7c46-263e-4902-dc93-3714c13fd1f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "well folks, my mac plus finally gave up the ghost this weekend after\n",
            "starting life as a 512k way back in 1985.  sooo, i'm in the market for a\n",
            "new machine a bit sooner than i intended to be...\n",
            "\n",
            "i'm looking into picking up a powerbook 160 or maybe 180 and have a bunch\n",
            "of questions that (hopefully) somebody can answer:\n",
            "\n",
            "* does anybody know any dirt on when the next round of powerbook\n",
            "introductions are expected?  i'd heard the 185c was supposed to make an\n",
            "appearence \"this summer\" but haven't heard anymore on it - and since i\n",
            "don't have access to macleak, i was wondering if anybody out there had\n",
            "more info...\n",
            "\n",
            "* has anybody heard rumors about price drops to the powerbook line like the\n",
            "ones the duo's just went through recently?\n",
            "\n",
            "* what's the impression of the display on the 180?  i could probably swing\n",
            "a 180 if i got the 80Mb disk rather than the 120, but i don't really have\n",
            "a feel for how much \"better\" the display is (yea, it looks great in the\n",
            "store, but is that all \"wow\" or is it really that good?).  could i solicit\n",
            "some opinions of people who use the 160 and 180 day-to-day on if its worth\n",
            "taking the disk size and money hit to get the active display?  (i realize\n",
            "this is a real subjective question, but i've only played around with the\n",
            "machines in a computer store breifly and figured the opinions of somebody\n",
            "who actually uses the machine daily might prove helpful).\n",
            "\n",
            "* how well does hellcats perform?  ;)\n",
            "\n",
            "thanks a bunch in advance for any info - if you could email, i'll post a\n",
            "summary (news reading time is at a premium with finals just around the\n",
            "corner... :( )\n",
            "--\n",
            "Tom Willis  \\  twillis@ecn.purdue.edu    \\    Purdue Electrical Engineering\n"
          ]
        }
      ],
      "source": [
        "first_train_text = dataset['train']['text'][2]\n",
        "print(first_train_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ma-sn4W30OZk",
        "outputId": "8bc58fb2-ff48-4115-93ed-0d78b33e631a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "well folks my mac plus finally gave up the ghost this weekend after starting life as a 512k way back in 1985 sooo im in the market for a new machine a bit sooner than i intended to be im looking into picking up a powerbook 160 or maybe 180 and have a bunch of questions that hopefully somebody can answer does anybody know any dirt on when the next round of powerbook introductions are expected id heard the 185c was supposed to make an appearence this summer but havent heard anymore on it and since i dont have access to macleak i was wondering if anybody out there had more info has anybody heard rumors about price drops to the powerbook line like the ones the duos just went through recently whats the impression of the display on the 180 i could probably swing a 180 if i got the 80Mb disk rather than the 120 but i dont really have a feel for how much better the display is yea it looks great in the store but is that all wow or is it really that good could i solicit some opinions of people who use the 160 and 180 daytoday on if its worth taking the disk size and money hit to get the active display i realize this is a real subjective question but ive only played around with the machines in a computer store breifly and figured the opinions of somebody who actually uses the machine daily might prove helpful how well does hellcats perform thanks a bunch in advance for any info if you could email ill post a summary news reading time is at a premium with finals just around the corner Tom Willis Purdue Electrical Engineering\n"
          ]
        }
      ],
      "source": [
        "# Define function for text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    # Remove email IDs\n",
        "    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
        "\n",
        "    # Remove special characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to the entire dataset\n",
        "for split in dataset.keys():\n",
        "    dataset[split] = dataset[split].map(lambda example: {'text': preprocess_text(example['text'])})\n",
        "\n",
        "# Example of accessing preprocessed data\n",
        "print(dataset['train']['text'][2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPHv8CJv0OZl"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataset import TensorDataset\n",
        "from avalanche.benchmarks.utils import AvalancheDataset\n",
        "\n",
        "# Step 1: Extract text and labels from your dataset\n",
        "texts_train = [example['text'] for example in dataset['train']]\n",
        "labels_train = [example['label'] for example in dataset['train']]\n",
        "\n",
        "texts_test = [example['text'] for example in dataset['test']]\n",
        "labels_test = [example['label'] for example in dataset['test']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Lb9lOSD0OZl"
      },
      "outputs": [],
      "source": [
        "def load_vocab(vocab_file):\n",
        "    vocab = {}\n",
        "    with open(vocab_file, 'r', encoding='utf-8') as f:\n",
        "        for index, token in enumerate(f.readlines()):\n",
        "            vocab[token.strip()] = index\n",
        "    return vocab\n",
        "\n",
        "vocab_file = 'vocab.txt'\n",
        "vocab = load_vocab(vocab_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybkNP1Ms0OZl"
      },
      "outputs": [],
      "source": [
        "class CustomTokenizer:\n",
        "    def __init__(self, vocab, max_length):\n",
        "        self.vocab = vocab\n",
        "        self.max_length = max_length\n",
        "        self.init_special_tokens()\n",
        "\n",
        "    def init_special_tokens(self):\n",
        "        self.cls_token_id = self.vocab.get(\"[CLS]\", 101)\n",
        "        self.sep_token_id = self.vocab.get(\"[SEP]\", 102)\n",
        "        self.unk_token_id = self.vocab.get(\"[UNK]\", 100)\n",
        "        self.pad_token_id = self.vocab.get(\"[PAD]\", 0)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        # Convert text to lowercase for uncased model\n",
        "        text = text.lower()\n",
        "        tokens = []\n",
        "        for word in text.split():\n",
        "            word_tokens = self.tokenize_word(word)\n",
        "            if not word_tokens:\n",
        "                tokens.append('[UNK]')\n",
        "            else:\n",
        "                tokens.extend(word_tokens)\n",
        "        return tokens\n",
        "\n",
        "    def tokenize_word(self, word):\n",
        "        \"\"\"Attempts to break down a word into the longest possible subwords known to the vocab.\"\"\"\n",
        "        if word in self.vocab:\n",
        "            return [word]\n",
        "\n",
        "        subwords = []\n",
        "        while word:\n",
        "            subword = self.find_longest_subword(word)\n",
        "            if subword:\n",
        "                subwords.append(subword)\n",
        "                word = word[len(subword):]\n",
        "                if word and word not in self.vocab:  # Add '##' prefix to the remaining subwords\n",
        "                    word = '##' + word\n",
        "            else:\n",
        "                return []  # Return empty if no subword is found, to be replaced by [UNK]\n",
        "        return subwords\n",
        "\n",
        "    def find_longest_subword(self, word):\n",
        "        for i in range(len(word), 0, -1):\n",
        "            if word[:i] in self.vocab:\n",
        "                return word[:i]\n",
        "        return None  # No subword found\n",
        "\n",
        "    def encode(self, text):\n",
        "        tokens = self.tokenize(text)\n",
        "        token_ids = [self.vocab.get(token, self.unk_token_id) for token in tokens]\n",
        "        token_ids = [self.cls_token_id] + token_ids + [self.sep_token_id]  # Add special tokens\n",
        "        return self.pad_or_truncate(token_ids)\n",
        "\n",
        "    def pad_or_truncate(self, token_ids):\n",
        "        \"\"\"Truncate the sequence to the specified max_length without adding padding.\"\"\"\n",
        "        # Truncate if the sequence is too long\n",
        "        if len(token_ids) > self.max_length:\n",
        "            token_ids = token_ids[:self.max_length]\n",
        "            # Ensure that the last token is always [SEP] if truncated\n",
        "            token_ids[-1] = self.sep_token_id\n",
        "        # No padding is added if the sequence is shorter than max_length\n",
        "        return token_ids\n",
        "\n",
        "    def __call__(self, texts, return_tensors='pt'):\n",
        "        all_encoded_ids = [self.encode(text) for text in texts]\n",
        "        if return_tensors == 'pt':\n",
        "            import torch\n",
        "            return torch.tensor(all_encoded_ids)\n",
        "        else:\n",
        "            return all_encoded_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zn7vZT2O0OZl"
      },
      "outputs": [],
      "source": [
        "max_length = 512  # Maximum sequence length\n",
        "custom_tokenizer = CustomTokenizer(vocab, max_length)\n",
        "# Comparing outputs\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load BERT tokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "text_example = texts_train[4]\n",
        "# Assuming custom_tokenizer is already created and max_length is set\n",
        "custom_encoded_text = custom_tokenizer([text_example], return_tensors='pt')\n",
        "\n",
        "# Tokenize with BERT tokenizer, using matching max_length and other parameters for fairness\n",
        "bert_encoded_text = bert_tokenizer([text_example], padding=True, max_length=max_length, return_tensors='pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2vjMvk50OZl",
        "outputId": "87f5cc10-d943-42e2-a797-433d67e81e1b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'From article by Tom A Baker My understanding is that the expected errors are basically known bugs in the warning system software things are checked that dont have the right values in yet because they arent set till after launch and suchlike Rather than fix the code and possibly introduce new bugs they just tell the crew ok if you see a warning no 213 before liftoff ignore it'"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyNinwhh0OZl",
        "outputId": "66494c6e-8917-46d4-a2cb-e5fe2ae976b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Custom Tokenizer Encoded Text:\n",
            " tensor([[  101,  2013,  3720,  2011,  3419,  1037,  6243,  2026,  4824,  2003,\n",
            "          2008,  1996,  3517, 10697,  2024, 10468,  2124, 12883,  1999,  1996,\n",
            "          5432,  2291,  4007,  2477,  2024,  7039,  2008,  2123,  1056,  2031,\n",
            "          1996,  2157,  5300,  1999,  2664,  2138,  2027,  4995,  1056,  2275,\n",
            "          6229,  2044,  4888,  1998,  2107,  2066,  2738,  2084,  8081,  1996,\n",
            "          3642,  1998,  4298,  8970,  2047, 12883,  2027,  2074,  2425,  1996,\n",
            "          3626,  7929,  2065,  2017,  2156,  1037,  5432,  2053, 19883,  2077,\n",
            "          6336,  2125,  8568,  2009,   102]])\n"
          ]
        }
      ],
      "source": [
        "print(\"Custom Tokenizer Encoded Text:\\n\", custom_encoded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kz-ZlHUQ0OZm",
        "outputId": "ff1be4df-40c2-4312-87f6-e80f0c74a91c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "BERT Tokenizer Encoded Text (input_ids):\n",
            " tensor([[  101,  2013,  3720,  2011,  3419,  1037,  6243,  2026,  4824,  2003,\n",
            "          2008,  1996,  3517, 10697,  2024, 10468,  2124, 12883,  1999,  1996,\n",
            "          5432,  2291,  4007,  2477,  2024,  7039,  2008,  2123,  2102,  2031,\n",
            "          1996,  2157,  5300,  1999,  2664,  2138,  2027,  4995,  2102,  2275,\n",
            "          6229,  2044,  4888,  1998,  2107, 10359,  2738,  2084,  8081,  1996,\n",
            "          3642,  1998,  4298,  8970,  2047, 12883,  2027,  2074,  2425,  1996,\n",
            "          3626,  7929,  2065,  2017,  2156,  1037,  5432,  2053, 19883,  2077,\n",
            "          6336,  7245,  8568,  2009,   102]])\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nBERT Tokenizer Encoded Text (input_ids):\\n\", bert_encoded_text['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eswqHrM90OZm",
        "outputId": "92c06194-680c-4618-a4a9-f9e4d19a7c87"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Tokenize text data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m encoded_texts_train \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m encoded_texts_test \u001b[38;5;241m=\u001b[39m custom_tokenizer(texts_test, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Convert labels to tensors\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[22], line 63\u001b[0m, in \u001b[0;36mCustomTokenizer.__call__\u001b[0;34m(self, texts, return_tensors)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m     all_encoded_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[22], line 48\u001b[0m, in \u001b[0;36mCustomTokenizer.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m---> 48\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mget(token, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munk_token_id) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[1;32m     50\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_token_id] \u001b[38;5;241m+\u001b[39m token_ids \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msep_token_id]  \u001b[38;5;66;03m# Add special tokens\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[22], line 17\u001b[0m, in \u001b[0;36mCustomTokenizer.tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     15\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit():\n\u001b[0;32m---> 17\u001b[0m     word_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m word_tokens:  \u001b[38;5;66;03m# If the word cannot be tokenized, use [UNK]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         tokens\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[22], line 31\u001b[0m, in \u001b[0;36mCustomTokenizer.tokenize_word\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     29\u001b[0m subwords \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m word:\n\u001b[0;32m---> 31\u001b[0m     subword \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_longest_subword\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subword:\n\u001b[1;32m     33\u001b[0m         subwords\u001b[38;5;241m.\u001b[39mappend(subword)\n",
            "Cell \u001b[0;32mIn[22], line 43\u001b[0m, in \u001b[0;36mCustomTokenizer.find_longest_subword\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_longest_subword\u001b[39m(\u001b[38;5;28mself\u001b[39m, word):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(word), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m word[:i] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab:\n\u001b[1;32m     44\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m word[:i]\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Tokenize text data\n",
        "encoded_texts_train = custom_tokenizer(texts_train, return_tensors='pt')\n",
        "encoded_texts_test = custom_tokenizer(texts_test, return_tensors='pt')\n",
        "\n",
        "# Convert labels to tensors\n",
        "labels_train_tensor = torch.tensor(labels_train)\n",
        "labels_test_tensor = torch.tensor(labels_test)\n",
        "\n",
        "# Create TensorDataset\n",
        "train_dataset = TensorDataset(encoded_texts_train, labels_train_tensor)\n",
        "test_dataset = TensorDataset(encoded_texts_test, labels_test_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TSwdQbJ0OZm"
      },
      "outputs": [],
      "source": [
        "# from transformers import BertTokenizer\n",
        "\n",
        "\n",
        "# # Step 2: Tokenize text data\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# encoded_texts_train = tokenizer(texts_train, padding=True, truncation=True, return_tensors='pt')\n",
        "# encoded_texts_test = tokenizer(texts_test, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# # Step 3: Convert labels to tensors\n",
        "# labels_train_tensor = torch.tensor(labels_train)\n",
        "# labels_test_tensor = torch.tensor(labels_test)\n",
        "\n",
        "# # Step 4: Create TensorDataset\n",
        "# train_dataset = TensorDataset(encoded_texts_train['input_ids'], encoded_texts_train['attention_mask'], labels_train_tensor)\n",
        "# test_dataset = TensorDataset(encoded_texts_test['input_ids'], encoded_texts_test['attention_mask'], labels_test_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0MhC1TQ0OZm"
      },
      "outputs": [],
      "source": [
        "avl_train_data = AvalancheDataset(train_dataset)\n",
        "avl_test_data = AvalancheDataset(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEOX8T4a0OZm"
      },
      "outputs": [],
      "source": [
        "avl_train_data.targets = labels_train\n",
        "avl_test_data.targets = labels_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jadY1tVL0OZm"
      },
      "outputs": [],
      "source": [
        "benchmark = nc_benchmark(\n",
        "    test_dataset=avl_test_data,  # Your Avalanche dataset\n",
        "    train_dataset=avl_train_data,\n",
        "    n_experiences=5,  # Number of experiences\n",
        "    task_labels=True  # Indicate that you have task labels for each experience\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5FTIEm70OZm"
      },
      "outputs": [],
      "source": [
        "train_stream = benchmark.train_stream\n",
        "experience = train_stream[0]\n",
        "\n",
        "# task label and dataset are the main attributes\n",
        "t_label = experience.task_label\n",
        "dataset = experience.dataset\n",
        "\n",
        "# but you can recover additional info\n",
        "print(experience.current_experience)\n",
        "print(experience.classes_in_this_experience)\n",
        "print(experience.classes_seen_so_far)\n",
        "print(experience.previous_classes)\n",
        "print(experience.future_classes)\n",
        "print(experience.origin_stream)\n",
        "print(experience.benchmark)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}