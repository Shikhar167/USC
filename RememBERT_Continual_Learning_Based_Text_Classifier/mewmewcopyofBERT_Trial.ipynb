{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7m5iPfVANhfa"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from typing import Union, Tuple, List, Iterable, Dict\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "import numpy as np\n",
        "import gzip, csv\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install datasets"
      ],
      "metadata": {
        "id": "45kZv80K968l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers\n",
        "from transformers import AutoTokenizer\n",
        "# If you can not find all the bugs, use the line below for AutoModel\n",
        "#from transformers import AutoModel\n"
      ],
      "metadata": {
        "id": "5pPDJEnvNsjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install avalanche-lib"
      ],
      "metadata": {
        "id": "BbyQ_X_a7ma3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install torch transformers"
      ],
      "metadata": {
        "id": "W8ObZdvN8F5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\"Implementation of the gelu activation function.\"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "class Config(object):\n",
        "    \"\"\"Configuration class to store the configuration of a `BertModel`.\"\"\"\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 hidden_size=512,\n",
        "                 num_hidden_layers=6,\n",
        "                 num_attention_heads=8,\n",
        "                 intermediate_size=2048,\n",
        "                 dropout_prob=0.1,\n",
        "                 max_position_embeddings=512,\n",
        "                 type_vocab_size=2,\n",
        "                 initializer_range=0.02):\n",
        "        \"\"\"Constructs Config for BertModel.\"\"\"\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.hidden_dropout_prob = dropout_prob\n",
        "        self.attention_probs_dropout_prob = dropout_prob\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.type_vocab_size = type_vocab_size\n",
        "        self.initializer_range = initializer_range\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, dict_object):\n",
        "        \"\"\"Constructs Config from a Python dictionary of parameters.\"\"\"\n",
        "        config = Config(vocab_size=None)\n",
        "        for (key, value) in dict_object.items():\n",
        "            config.__dict__[key] = value\n",
        "        return config\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"Layer normalization module.\"\"\"\n",
        "    def __init__(self, hidden_size, variance_epsilon=1e-12):\n",
        "        \"\"\"Constructs LayerNorm object for Transformer layer in BERT model.\"\"\"\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.beta = nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.variance_epsilon = variance_epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass of the LayerNorm layer.\"\"\"\n",
        "        u = x.mean(-1, keepdim=True)\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "        return self.gamma * x + self.beta\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"Feed forward network with gelu activation.\"\"\"\n",
        "    def __init__(self, hidden_size, intermediate_size):\n",
        "        \"\"\"Constructs MLP object for Transformer layer in BERT model.\"\"\"\n",
        "        super(MLP, self).__init__()\n",
        "        self.dense_expansion = nn.Linear(hidden_size, intermediate_size)\n",
        "        self.dense_contraction = nn.Linear(intermediate_size, hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass of the MLP layer.\"\"\"\n",
        "        x = self.dense_expansion(x)\n",
        "        x = self.dense_contraction(gelu(x))\n",
        "        return x\n",
        "\n",
        "class Layer(nn.Module):\n",
        "    \"\"\"The Transformer layer.\"\"\"\n",
        "    def __init__(self, config):\n",
        "        \"\"\"Constructs Layer object for Transformer layer in BERT model based on config.\"\"\"\n",
        "        super(Layer, self).__init__()\n",
        "\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "        self.attn_out = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.ln1 = LayerNorm(config.hidden_size)\n",
        "\n",
        "        self.mlp = MLP(config.hidden_size, config.intermediate_size)\n",
        "        self.ln2 = LayerNorm(config.hidden_size)\n",
        "\n",
        "    def split_heads(self, tensor, num_heads, attention_head_size):\n",
        "        \"\"\"Split hidden_size into num_heads * attention_head_size and transpose into shape [batch, num_heads, seq_len, attention_head_size].\"\"\"\n",
        "        new_shape = tensor.size()[:-1] + (num_heads, attention_head_size)\n",
        "        tensor = tensor.view(*new_shape)\n",
        "        return tensor.permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "    def merge_heads(self, tensor, num_heads, attention_head_size):\n",
        "        \"\"\"Transpose and then reshape into shape [batch, seq_len, hidden_size].\"\"\"\n",
        "        tensor = tensor.permute(0, 2, 1, 3).contiguous()\n",
        "        new_shape = tensor.size()[:-2] + (num_heads * attention_head_size,)\n",
        "        return tensor.view(new_shape)\n",
        "\n",
        "    def attn(self, q, k, v, attention_mask):\n",
        "        \"\"\"Attention mechanism for the Transformer layer.\"\"\"\n",
        "        mask = attention_mask == 1\n",
        "        mask = mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        s = torch.matmul(q, k.transpose(-1, -2))\n",
        "        s = s / math.sqrt(self.attention_head_size)\n",
        "\n",
        "        s = torch.where(mask, s, torch.tensor(float('-inf')))\n",
        "\n",
        "        p = F.softmax(s, dim=-1)\n",
        "        p = self.dropout(p)\n",
        "\n",
        "        a = torch.matmul(p, v)\n",
        "        return a\n",
        "\n",
        "    def forward(self, x, attention_mask):\n",
        "        \"\"\"Forward pass of the Transformer layer in BERT.\"\"\"\n",
        "        q, k, v = self.query(x), self.key(x), self.value(x)\n",
        "\n",
        "        q = self.split_heads(q, self.num_attention_heads, self.attention_head_size)\n",
        "        k = self.split_heads(k, self.num_attention_heads, self.attention_head_size)\n",
        "        v = self.split_heads(v, self.num_attention_heads, self.attention_head_size)\n",
        "\n",
        "        a = self.attn(q, k, v, attention_mask)\n",
        "        a = self.merge_heads(a, self.num_attention_heads, self.attention_head_size)\n",
        "        a = self.attn_out(a)\n",
        "        a = self.dropout(a)\n",
        "        a = self.ln1(a + x)\n",
        "\n",
        "        m = self.mlp(a)\n",
        "        m = self.dropout(m)\n",
        "        m = self.ln2(m + a)\n",
        "\n",
        "        return m\n",
        "\n",
        "class Bert(nn.Module):\n",
        "    def __init__(self, config_dict):\n",
        "        super(Bert, self).__init__()\n",
        "        self.config = Config.from_dict(config_dict)  # Create an instance of Config\n",
        "\n",
        "        self.embeddings = nn.ModuleDict({\n",
        "            'token': nn.Embedding(self.config.vocab_size, self.config.hidden_size, padding_idx=0),\n",
        "            'position': nn.Embedding(self.config.max_position_embeddings, self.config.hidden_size),\n",
        "            'token_type': nn.Embedding(self.config.type_vocab_size, self.config.hidden_size),\n",
        "        })\n",
        "\n",
        "        self.ln = LayerNorm(self.config.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            Layer(self.config) for _ in range(self.config.num_hidden_layers)\n",
        "        ])\n",
        "\n",
        "        self.pooler = nn.Sequential(OrderedDict([\n",
        "            ('dense', nn.Linear(self.config.hidden_size, self.config.hidden_size)),\n",
        "            ('activation', nn.Tanh()),\n",
        "        ]))\n",
        "\n",
        "        # Add a classifier layer for classification\n",
        "        self.classifier = nn.Linear(self.config.hidden_size, 20)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
        "        position_ids = torch.arange(input_ids.size(1), dtype=torch.long, device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        x = (self.embeddings['token'](input_ids) +\n",
        "             self.embeddings['position'](position_ids) +\n",
        "             self.embeddings['token_type'](token_type_ids))\n",
        "        x = self.dropout(self.ln(x))\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, attention_mask)\n",
        "\n",
        "        o = self.pooler(x[:, 0])\n",
        "\n",
        "        if labels is not None:\n",
        "          print(\"HELLO WORLD\")\n",
        "          # Use the classifier layer for classification\n",
        "          logits = F.softmax(self.classifier(o), dim=1)\n",
        "          #loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "          return logits\n",
        "\n",
        "        return x, o\n",
        "\n",
        "    def save_model(self, path):\n",
        "        \"\"\"Save model to a file.\n",
        "\n",
        "        Args:\n",
        "            path (str): Path to the file where the model will be saved.\n",
        "        \"\"\"\n",
        "        torch.save(self.state_dict(), path)\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, config_dict, path):\n",
        "        \"\"\"Load model from a file.\n",
        "\n",
        "        Args:\n",
        "            config_dict (dict): Dictionary containing the configuration of the model.\n",
        "            path (str): Path to the model checkpoint.\n",
        "\n",
        "        Returns:\n",
        "            Bert: Bert model loaded from the checkpoint.\n",
        "        \"\"\"\n",
        "        model = cls(config_dict)\n",
        "        model.load_state_dict(torch.load(path))\n",
        "        return model\n"
      ],
      "metadata": {
        "id": "xwliFn7kPeBH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Define your custom BERT configuration\n",
        "config_dict = {\n",
        "    'vocab_size': 310000,\n",
        "    'hidden_size': 512,\n",
        "    'num_attention_heads': 2,\n",
        "    'num_hidden_layers': 4,\n",
        "    'intermediate_size': 512,\n",
        "    'dropout_prob': 0.1,\n",
        "    'max_position_embeddings': 512,\n",
        "    'type_vocab_size': 2,\n",
        "    'initializer_range': 0.02\n",
        "}\n",
        "\n",
        "# Create an instance of your custom BERT model\n",
        "model = Bert(config_dict)\n",
        "\n",
        "# Tokenizer for BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # Replace with your desired tokenizer\n",
        "\n",
        "# Define a DataLoader for the dataset\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        encoding = self.tokenizer(item['text'], truncation=True, padding='max_length', return_tensors='pt', max_length=512)\n",
        "        encoding['label'] = torch.tensor(item['label'])\n",
        "        return encoding\n",
        "\n",
        "# Use DynamicPaddingCollate for DataLoader\n",
        "class DynamicPaddingCollate:\n",
        "    def __call__(self, batch):\n",
        "        return {\n",
        "            'input_ids': torch.stack([sample['input_ids'].squeeze(0) for sample in batch]),\n",
        "            'attention_mask': torch.stack([sample['attention_mask'].squeeze(0) for sample in batch]),\n",
        "            'labels': torch.tensor([sample['label'] for sample in batch])\n",
        "        }\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"setfit/20_newsgroups\", split=\"train\")\n",
        "\n",
        "# Create DataLoader with DynamicPaddingCollate\n",
        "train_dataset = MyDataset(dataset, tokenizer)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=DynamicPaddingCollate())\n",
        "\n",
        "# Training parameters\n",
        "epochs = 2\n",
        "learning_rate = 2e-5\n",
        "\n",
        "# Set up optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Training loop\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_dataloader, 1):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids =batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        # print(outputs)\n",
        "        # check = outputs\n",
        "        # break\n",
        "        #loss =\n",
        "        loss = criterion(outputs, labels)  # Assuming your forward method returns the loss directly\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 10 == 0:  # Print every 10 batches\n",
        "            avg_loss = total_loss / batch_idx\n",
        "            print(f\"Batch {batch_idx}/{len(train_dataloader)} - Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    avg_epoch_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1} - Avg Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LVi3aTlg1HxT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e39a090e-4e9a-47ab-988c-cdd0dabf53cc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
            "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "Batch 10/354 - Avg Loss: 2.9962\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "Batch 20/354 - Avg Loss: 2.9954\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "Batch 30/354 - Avg Loss: 2.9950\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n",
            "Batch 40/354 - Avg Loss: 2.9953\n",
            "HELLO WORLD\n",
            "HELLO WORLD\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-ce884abfcb1b>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m#loss =\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Assuming your forward method returns the loss directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of target with class indices\n",
        "loss = nn.CrossEntropyLoss()\n",
        "input = torch.randn(3, 5, requires_grad=True)\n",
        "target = torch.empty(3, dtype=torch.long).random_(5)\n",
        "print(input)\n",
        "print(target)\n",
        "output = loss(input, target)\n",
        "print(\"one\",output)\n",
        "output.backward()\n",
        "print(\"one\",output)\n",
        "#Example of target with class probabilities\n",
        "# input = torch.randn(3, 5, requires_grad=True)\n",
        "# target = torch.randn(3, 5).softmax(dim=1)\n",
        "# print(target)\n",
        "# output = loss(input, target)\n",
        "# output.backward()\n",
        "# print(\"two\",output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ehNWq2TCNif",
        "outputId": "6309e4f8-7fa1-44f7-d214-795cd3965eaf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.2599,  1.7095, -1.8641,  0.4603,  1.0727],\n",
            "        [ 1.3016,  0.2321, -1.3055, -1.0979,  0.3024],\n",
            "        [-0.4994,  0.4463, -2.0962,  1.7518,  1.2974]], requires_grad=True)\n",
            "tensor([2, 4, 0])\n",
            "one tensor(2.9490, grad_fn=<NllLossBackward0>)\n",
            "one tensor(2.9490, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er3P-1WYAfjF",
        "outputId": "e440308d-2f31-462e-a8e3-82cc68828731"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0563, 0.0574, 0.0524, 0.0411, 0.0941, 0.0515, 0.0538, 0.0506, 0.0443,\n",
              "         0.0489, 0.0426, 0.0487, 0.0561, 0.0322, 0.0470, 0.0469, 0.0438, 0.0522,\n",
              "         0.0449, 0.0352],\n",
              "        [0.0521, 0.0482, 0.0492, 0.0369, 0.0977, 0.0582, 0.0564, 0.0400, 0.0496,\n",
              "         0.0411, 0.0519, 0.0455, 0.0738, 0.0366, 0.0482, 0.0340, 0.0500, 0.0406,\n",
              "         0.0575, 0.0323],\n",
              "        [0.0590, 0.0494, 0.0491, 0.0429, 0.0783, 0.0548, 0.0565, 0.0452, 0.0483,\n",
              "         0.0544, 0.0437, 0.0416, 0.0769, 0.0388, 0.0457, 0.0445, 0.0484, 0.0401,\n",
              "         0.0505, 0.0319],\n",
              "        [0.0510, 0.0491, 0.0431, 0.0398, 0.1017, 0.0448, 0.0531, 0.0348, 0.0533,\n",
              "         0.0454, 0.0487, 0.0429, 0.0787, 0.0329, 0.0516, 0.0436, 0.0493, 0.0456,\n",
              "         0.0612, 0.0293],\n",
              "        [0.0507, 0.0465, 0.0489, 0.0376, 0.0945, 0.0496, 0.0660, 0.0351, 0.0511,\n",
              "         0.0464, 0.0428, 0.0453, 0.0753, 0.0330, 0.0497, 0.0458, 0.0557, 0.0445,\n",
              "         0.0510, 0.0305],\n",
              "        [0.0600, 0.0488, 0.0499, 0.0383, 0.0946, 0.0540, 0.0610, 0.0395, 0.0469,\n",
              "         0.0532, 0.0476, 0.0485, 0.0820, 0.0330, 0.0413, 0.0391, 0.0535, 0.0310,\n",
              "         0.0499, 0.0279],\n",
              "        [0.0489, 0.0439, 0.0467, 0.0416, 0.0943, 0.0537, 0.0599, 0.0392, 0.0585,\n",
              "         0.0528, 0.0411, 0.0409, 0.0746, 0.0355, 0.0443, 0.0395, 0.0502, 0.0393,\n",
              "         0.0647, 0.0302],\n",
              "        [0.0450, 0.0477, 0.0466, 0.0373, 0.0984, 0.0515, 0.0618, 0.0377, 0.0500,\n",
              "         0.0607, 0.0452, 0.0451, 0.0809, 0.0334, 0.0414, 0.0428, 0.0504, 0.0415,\n",
              "         0.0537, 0.0289],\n",
              "        [0.0615, 0.0489, 0.0472, 0.0438, 0.0879, 0.0470, 0.0739, 0.0403, 0.0482,\n",
              "         0.0489, 0.0381, 0.0489, 0.0709, 0.0342, 0.0502, 0.0425, 0.0504, 0.0368,\n",
              "         0.0482, 0.0322],\n",
              "        [0.0448, 0.0530, 0.0487, 0.0432, 0.0992, 0.0609, 0.0726, 0.0396, 0.0377,\n",
              "         0.0541, 0.0418, 0.0428, 0.0710, 0.0379, 0.0451, 0.0425, 0.0486, 0.0420,\n",
              "         0.0485, 0.0259],\n",
              "        [0.0649, 0.0406, 0.0445, 0.0385, 0.0790, 0.0498, 0.0620, 0.0404, 0.0427,\n",
              "         0.0515, 0.0569, 0.0477, 0.0775, 0.0360, 0.0456, 0.0455, 0.0603, 0.0382,\n",
              "         0.0502, 0.0283],\n",
              "        [0.0586, 0.0558, 0.0450, 0.0449, 0.0967, 0.0518, 0.0527, 0.0399, 0.0486,\n",
              "         0.0436, 0.0445, 0.0396, 0.0654, 0.0322, 0.0480, 0.0440, 0.0549, 0.0493,\n",
              "         0.0516, 0.0330],\n",
              "        [0.0517, 0.0526, 0.0545, 0.0397, 0.0858, 0.0524, 0.0624, 0.0374, 0.0548,\n",
              "         0.0429, 0.0535, 0.0376, 0.0776, 0.0322, 0.0498, 0.0378, 0.0587, 0.0323,\n",
              "         0.0592, 0.0269],\n",
              "        [0.0441, 0.0534, 0.0498, 0.0380, 0.0882, 0.0566, 0.0672, 0.0426, 0.0539,\n",
              "         0.0511, 0.0457, 0.0451, 0.0812, 0.0321, 0.0451, 0.0381, 0.0420, 0.0387,\n",
              "         0.0560, 0.0314],\n",
              "        [0.0500, 0.0464, 0.0529, 0.0358, 0.0988, 0.0567, 0.0589, 0.0387, 0.0481,\n",
              "         0.0572, 0.0446, 0.0440, 0.0755, 0.0399, 0.0464, 0.0356, 0.0474, 0.0412,\n",
              "         0.0504, 0.0314],\n",
              "        [0.0485, 0.0453, 0.0478, 0.0307, 0.0969, 0.0684, 0.0609, 0.0353, 0.0487,\n",
              "         0.0528, 0.0520, 0.0428, 0.0726, 0.0331, 0.0448, 0.0403, 0.0512, 0.0362,\n",
              "         0.0635, 0.0283],\n",
              "        [0.0415, 0.0415, 0.0552, 0.0385, 0.1105, 0.0488, 0.0568, 0.0371, 0.0511,\n",
              "         0.0551, 0.0448, 0.0455, 0.0663, 0.0346, 0.0438, 0.0508, 0.0494, 0.0432,\n",
              "         0.0521, 0.0335],\n",
              "        [0.0536, 0.0524, 0.0456, 0.0365, 0.1130, 0.0546, 0.0625, 0.0381, 0.0456,\n",
              "         0.0419, 0.0466, 0.0412, 0.0730, 0.0370, 0.0412, 0.0445, 0.0507, 0.0419,\n",
              "         0.0504, 0.0296],\n",
              "        [0.0601, 0.0566, 0.0486, 0.0402, 0.0745, 0.0464, 0.0677, 0.0417, 0.0508,\n",
              "         0.0449, 0.0466, 0.0454, 0.0792, 0.0407, 0.0434, 0.0389, 0.0508, 0.0341,\n",
              "         0.0585, 0.0310],\n",
              "        [0.0548, 0.0545, 0.0464, 0.0322, 0.0898, 0.0498, 0.0599, 0.0493, 0.0480,\n",
              "         0.0460, 0.0446, 0.0438, 0.0709, 0.0375, 0.0463, 0.0431, 0.0548, 0.0392,\n",
              "         0.0543, 0.0347],\n",
              "        [0.0451, 0.0517, 0.0393, 0.0332, 0.0963, 0.0542, 0.0732, 0.0400, 0.0521,\n",
              "         0.0539, 0.0544, 0.0392, 0.0767, 0.0298, 0.0512, 0.0436, 0.0478, 0.0368,\n",
              "         0.0509, 0.0306],\n",
              "        [0.0622, 0.0447, 0.0520, 0.0373, 0.1000, 0.0474, 0.0619, 0.0359, 0.0552,\n",
              "         0.0447, 0.0386, 0.0469, 0.0778, 0.0366, 0.0438, 0.0359, 0.0523, 0.0386,\n",
              "         0.0572, 0.0310],\n",
              "        [0.0472, 0.0482, 0.0536, 0.0370, 0.0985, 0.0531, 0.0591, 0.0386, 0.0562,\n",
              "         0.0642, 0.0459, 0.0416, 0.0739, 0.0279, 0.0354, 0.0405, 0.0573, 0.0413,\n",
              "         0.0543, 0.0262],\n",
              "        [0.0605, 0.0452, 0.0480, 0.0341, 0.0792, 0.0537, 0.0726, 0.0402, 0.0533,\n",
              "         0.0601, 0.0409, 0.0432, 0.0679, 0.0405, 0.0418, 0.0439, 0.0532, 0.0436,\n",
              "         0.0503, 0.0278],\n",
              "        [0.0536, 0.0495, 0.0454, 0.0375, 0.0946, 0.0525, 0.0646, 0.0399, 0.0558,\n",
              "         0.0494, 0.0493, 0.0371, 0.0759, 0.0343, 0.0384, 0.0406, 0.0534, 0.0401,\n",
              "         0.0575, 0.0307],\n",
              "        [0.0547, 0.0504, 0.0533, 0.0372, 0.0924, 0.0515, 0.0546, 0.0311, 0.0509,\n",
              "         0.0490, 0.0424, 0.0487, 0.0835, 0.0338, 0.0414, 0.0393, 0.0494, 0.0419,\n",
              "         0.0670, 0.0274],\n",
              "        [0.0594, 0.0434, 0.0421, 0.0388, 0.0972, 0.0593, 0.0551, 0.0353, 0.0609,\n",
              "         0.0486, 0.0500, 0.0445, 0.0705, 0.0277, 0.0487, 0.0408, 0.0504, 0.0362,\n",
              "         0.0573, 0.0337],\n",
              "        [0.0481, 0.0426, 0.0556, 0.0372, 0.1081, 0.0580, 0.0579, 0.0331, 0.0498,\n",
              "         0.0455, 0.0423, 0.0469, 0.0739, 0.0342, 0.0402, 0.0499, 0.0436, 0.0418,\n",
              "         0.0546, 0.0366],\n",
              "        [0.0559, 0.0521, 0.0506, 0.0331, 0.0884, 0.0544, 0.0600, 0.0437, 0.0492,\n",
              "         0.0445, 0.0493, 0.0446, 0.0816, 0.0360, 0.0452, 0.0393, 0.0518, 0.0353,\n",
              "         0.0524, 0.0326],\n",
              "        [0.0508, 0.0458, 0.0600, 0.0349, 0.0944, 0.0487, 0.0590, 0.0410, 0.0522,\n",
              "         0.0572, 0.0461, 0.0453, 0.0849, 0.0361, 0.0392, 0.0416, 0.0509, 0.0374,\n",
              "         0.0472, 0.0272],\n",
              "        [0.0496, 0.0510, 0.0462, 0.0365, 0.1031, 0.0544, 0.0548, 0.0423, 0.0542,\n",
              "         0.0526, 0.0473, 0.0460, 0.0719, 0.0339, 0.0408, 0.0376, 0.0419, 0.0423,\n",
              "         0.0561, 0.0375],\n",
              "        [0.0514, 0.0488, 0.0572, 0.0355, 0.0942, 0.0562, 0.0554, 0.0337, 0.0495,\n",
              "         0.0610, 0.0475, 0.0413, 0.0772, 0.0289, 0.0440, 0.0377, 0.0541, 0.0400,\n",
              "         0.0544, 0.0320]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "magic = torch.argmax(outputs, dim = 1)\n",
        "magic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmJzqLGIAoSB",
        "outputId": "8e230dd2-f6fd-49fb-f158-3212be320ef1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
              "        12,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save the model\n",
        "model.save_model(\"/content/my_custom_bert_model.pth\")"
      ],
      "metadata": {
        "id": "c6DeXuo0AF8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model for predictions\n",
        "loaded_model = Bert.load_model(config_dict, \"/content/my_custom_bert_model.pth\")\n",
        "\n",
        "# Perform predictions\n",
        "sentence = \"This is a test sentence.\"\n",
        "input_ids = tokenizer(sentence, return_tensors='pt')['input_ids']\n",
        "output = loaded_model(input_ids)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "20NryXzZADIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3UyqatN5buz",
        "outputId": "2d2ba3a9-3425-4927-aab4-c26aa5faa8fd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0868, 0.0667, 0.0448, 0.0425, 0.0855, 0.0469, 0.0439, 0.0399, 0.0453,\n",
              "        0.0427, 0.0820, 0.0357, 0.0340, 0.0547, 0.0405, 0.0406, 0.0423, 0.0527,\n",
              "        0.0329, 0.0395], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = torch.argmax(check[0])\n",
        "prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_tKQzbI5vQf",
        "outputId": "32dbd1c6-e2aa-4a79-9438-9cb86f4674d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WcxT3HW6Cbp",
        "outputId": "8ec65fb7-9be6-4582-a81d-621535b04f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.3700,  0.4549,  0.0396,  0.1715,  0.2064,  0.1312,  0.3936, -0.0831,\n",
              "         0.2345, -0.2406, -0.2414,  0.3022, -0.1611, -0.3026, -0.0525,  0.0938,\n",
              "         0.2632, -0.0264,  0.3938,  0.2777], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(model.state_dict(), 'model_weights.pth')\n",
        "# model = MyModel()  # Make sure this is the same model architecture\n",
        "# model.load_state_dict(torch.load('model_weights.pth'))\n",
        "# model.eval()"
      ],
      "metadata": {
        "id": "AFxJolfz6O__",
        "outputId": "38c8913d-4cf6-4d7a-f0a5-ae9745b1b444",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([17,  4,  9, 13, 19, 16,  5,  1,  6,  6, 12,  3,  4, 19, 10,  4,  1, 14,\n",
              "         5, 13, 19, 17,  0, 19, 16, 12,  0, 10,  6,  3,  7,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the 19th row in the \"text\" column\n",
        "text_19th_row = dataset['train']['text'][18]\n",
        "\n",
        "print(text_19th_row)"
      ],
      "metadata": {
        "id": "SxPJfvlb_n5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_idx, batch in enumerate(train_dataloader, 1):\n",
        "  print(**batch)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "MyLKjtn48Iwk",
        "outputId": "87f1b2c2-7d2e-4f85-e129-86b509b4b36e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'input_ids' is an invalid keyword argument for print()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-b03af2fa4485>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'input_ids' is an invalid keyword argument for print()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S-vGz43W8KFf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}